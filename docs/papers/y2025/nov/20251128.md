import blog_20251127_img1 from './asset/blog_20251127_img1.png';

# 2025-11-28 papers review

## 1. [Video Generation Models Are Good Latent Reward Models](https://huggingface.co/papers/2511.21541)

Reward Feedback learning 방식은 human preference에 큰 도움이 됨. 하지만 image generation에서 많이 활용, video generation에서 적용하기 어려웠다.
- GPU memory bottleneck
- Evaluation Delay : reward models이 denoising을 요구하여 훈련 속도가 느려짐.
- Insufficient Supervision : reward 가 최종 결과에만 적용되어 early generation stage 가이드 능력이 떨어짐.
- VLM의 timestep generalization 부족 : timestep이 길어질수록 성능 저하.

VGM에서 reward modeling 활용. **Process-Award Video Reward(PAVRM)** 모델과 **Process Reward Feedback Learning (PRFL)** 프레임워크 제안.
> We propose a process-aware video reward model (PAVRM), which employs query-based aggregation to efficiently handle variable-length videos with timestep sensitivity, and keep artifact awareness throughout the denoising process based on pre-trained video models.
> We introduce process reward feedback learning (PRFL), an efficient video post-training framework that operates in latent space by sampling random timesteps and optimizing through single denoising steps, without VAE decoding and distributing reward across the full denoising process.

- PAVRM은 fixed-size reward score를 예측.
- **VAE decoding 없이** latent space에서 random timestep을 샘플링하고, process reward를 계산하여 VGM을 최적화.

## 2. [What does it mean to understand language?](https://huggingface.co/papers/2511.19757)

언어 이해는 단순히 표면적 의미를 추출하는 것을 넘어, 상황에 대한 풍분한 정신 모델을 구축해야 함. 수십년간 행동 연구는 simulation models 구축을 지지했다. 뇌의 핵심 언어 시스템은 통계 기반의 shallow understanding만 수행하는 것 같다. 언어의 본질에 대한 다양한 논문과 설명이 그렇게 얘기한다. 비트게슈타인. 하지만 사람의 뇌는 언어 기능 뿐 아니라 다른 인지 시스템과 연결되어 deep understanding을 수행한다.

그러므로 LLM 모델 역시 인지 시스템과 교류가 있어야 한다. 이런 인지 시스템에는 이론적인 사고, 직관적인 추론, 지각/운동 표상 시스템 등이 포함된 영역이다. 

## 3. [ENACT](https://huggingface.co/papers/2511.20937)
### Evaluating Embodied Cognition with World Modeling of Egocentric Interaction

VLA가 아닌 VLM 자체가 가지고 있는 embodied cognition 능력을 확인하기 위한 벤츠마크 및 프레임 워크 제안.

VLA는 목적 달성을 평가하지만, VLM은 World Modeling 및 Causual Reasoning을 반영한다. 이는 VLA의 기반이 되는 VLM의 근본적인 능력에 던지는 의문. 심지어 VLM은 non-embodied scenario를 상정하고 학습한다.
- 목적을 달성하기 위해 학습하는 모델이 세상을 제대로 이해하고 있긴 한가?

이를 위해서 VQA 기반의 벤치마크 도입. 상황->액션, 액션->상황 이해 능력을 long-horizon에서 평가하는 벤치마크를 구축함.

확인해본 결과 VLM의 상호작용이 길어질수록 성능이 급락함을 보여주고 있다. 즉, VLA를 학습시키 지너에 VLM 먼저 검토를 해야함을 시사한다.