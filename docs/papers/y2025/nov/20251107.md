import blog_20251107_img0 from './asset/blog_20251107_img0.png';
import blog_20251107_img1 from './asset/blog_20251107_img1.png';
import blog_20251107_img2 from './asset/blog_20251107_img2.png';
import blog_20251107_img3 from './asset/blog_20251107_img3.png';
import blog_20251107_img4 from './asset/blog_20251107_img4.png';

# 2025-11-07 papers review

## 1. [V-Thinker: Interactive Thinking with Images](https://huggingface.co/papers/2511.04460)

최근 비슷한 논문이 말하듯이, 생각하는 과정에서는 이미지 사고가 필요함. 언어 사고 이상으로 나아가야 함. 
- 기존 이미지 사고 논문들은 이미지를 사고의 보조 도구로 활용. 
- 본 논문은 모델이 이미지와 능동적으로 상호작용하며 생각하도록 학습시킴. 예) 이미지에 보조선 그리기.

image.png

본 논문 역시 이미지 사고를 사용한 RL - **V-Thinker**:
- 벤치마크 데이터셑 제작: VTBench
- 상호작용추론 데이터셑을 자동으로 생성, 검증하는 파이프라인 구축.
- Python 코드를 생성하여 이미지를 적극적으로 수정(edit) -> 수정된 이미지를 즉시 피드백받아 다음 추론을 이어가는 것.
  - 예시) 기하학 문제를 CoT로 푸는 도중에 파이썬 코드를 생성. 파이썬 코드를 통해 이미지 위에 보조선이나 점을 그린다. 이 이미지를 입력받아 추론을 이어감.

<div style={{textAlign: 'center'}}>
 <img src={blog_20251107_img2} style={{width: 500}} />
</div>

- 지각 능력부터 상호 추론 능력까지 2단계로 점진적으로 학습.
  - Perception-40K 데이터셋을 통해 점 수준(point-level)의 정확한 위치 파악 능력을 SFT로 학습
    - 특정 지점 좌표 인식 -> "정육면체의 상단 꼭짓점"과 같은 기하학적 이해 -> "정육면체의 중심 찾기" 처럼 지각과 계산이 결합된 작업.
  - V-Interaction-400K 데이터셋으로 SFT(콜드 스타트)를 진행한 후 , We-Math 2.0 등 다양한 데이터를 활용해 GRPO 강화학습을 수행

이미지 생성 프레임워크
<div style={{textAlign: 'center'}}>
 <img src={blog_20251107_img1} style={{width: 500}} />
</div>

Result:
- 성능 일관적으로 증가.
- Instruction-Guided Interaction에서 Qwen2.5-Vl-7B 대비 25.8% 성능 향상.


## 2. [Thinking with Video](https://huggingface.co/papers/2511.04570)
### Video Generation as a Promising Multimodal Reasoning Paradigm

비디오 기반 사고 과정을 Sora-2를 통해 생성. 재밌는 논문이다. 그리고 어찌보면 당연해보이는데 왜 생각못했지? 좋은 논문이라는 뜻이라고 생각.

미로 해결 문제를 입력으로 받으면, 모델은 **"미로의 출발점에서 도착점까지 붉은 선을 그려 나가는 동영상"**을 생성.

이 과정이 곧 추론 과정이자 정답임.

Vision-Centric 과제는 동영상을 통해 시각화하고,
Text-Centric 과제는 동영상 안에 텍스트를 직접 삽입하는 방식을 사용.

Text-Centric 작업은 SOTA LLM 보다 못했지만, Vision-Centric 과제에서는 SOTA VLM과 비슷하거나 더 좋은 성능.

<div style={{textAlign: 'center'}}>
 <img src={blog_20251107_img3} style={{width: 500}} />
</div>

<div style={{textAlign: 'center'}}>
 <img src={blog_20251107_img4} style={{width: 500}} />
</div>

## 1. [Learning Vision-Driven Reactive Soccer Skills for Humanoid Robots](https://huggingface.co/papers/2511.03996) `bytedance`

bytedance랑 칭화대가 협업한 논문. 근데 바이트댄스 로봇 쪽에도 관심이 있나?

휴머노이드 모듈은 보통 인지 모듈과 행동 모듈이 분리되어있다. 그러나 이런 구조는 느린 처리 속도로 스포츠에 적합하지 않음. 

논문에서는 인지와 모션 제어를 직접 통합하는 RL 기반 통합 컨트롤러 제안.
- 이 모듈은 영상 입력(공 위치), 골대 위치 Odometry 모듈, 그 외 샌서 정보를 state로 입력받아 관절 동작 정보를 action으로 출력하는 단일 컨트롤러.

<div style={{textAlign: 'center'}}>
 <img src={blog_20251107_img0} style={{width: 500}} />
</div>

결과:
- 행동 일관성과 민첩성이 다 높아진 컨트롤러 개발.

## 2. [GUI-360: A Comprehensive Dataset and Benchmark for Computer-Using Agents](https://huggingface.co/papers/2511.04307) `microsoft`

벤치마크 데이터를 구축하는 논문들의 전반적인 레시피의 구성을 따라갈만하다. **템플릿과 LLM을 활용한 데이터 생성 -> LLM을 이용한 데이터 평가**의 큰 틀.

데스크톱 환경 CUA 연구의 어려움 3가지 (당연히 웹 환경보다 어렵지만,):
- 실제 사용자 작업(real-world task) 데이터의 희소성.
- 스크린샷과 행동 궤적(trajectory)을 자동으로 수집/분석할 파이프라인의 부재. 
- GUI grounding, screen parsing, action prediction 통합 평가 벤치마크 전무.

**GUI-360°**라는 대규모 데이터셋과 벤치마크를 구축.
- LLM 기반 자동화 파이프라인으로 실제 사용자 의도 수집. 예) "파워포인트 글씨 굵게하는 법"
- 템플릿을 이용해서 테스크 구체화. 예) "파워포인트에서 Hello 라는 텍스트를 굵게 만드세요."
- TrajAgent를 구축. 태스크를 자동 수행하면 trajectory 데이터 수집.
  - TrajAgent 는 매우 무겁고 비싼 모델.
  - task를 수행하고, 최종적으로 이를 검사해서 성공/실패 여부를 판단.
  - GUI-360 데이터에는 성공/실패 정보가 모두 들어감. 

Result:
- Word, Excel, PowerPoint 전반에 걸쳐 120만 개 이상의 행동 스텝(action steps) 을 포함하는 대규모 데이터셋을 구축
- SOTA vlm 모델 벤치마킹 결과, out-of-box 성능이 매우 낮음.
- GUI-360 데이터로 SFT 한 결과 성능이 대폭 향상됨을 확인.


## 3. [NVIDIA Nemotron Nano V2 VL](https://huggingface.co/papers/2511.03929) `nvidia`

모델의 역할이 커짐에 따라 모델 context length가 점점 중요해진다. 

context length 확장이 어려운 이유:
- 모델 context length를 늘리고 비전 데이터를 학습하면 순수 텍스트 추론 능력이 저하된다.

Proposed Method - **Nemotron Nano V2**:
- Mamba-Transformer 기반 모델 개발.
- multi-stage SFT: 16K context train -> 49K video/multi image SFT -> 순수 텍스트 추론 능력 복구 -> 300K+ 장문 컨텍스트.

Result:
- 효율적인 12B VLM, Nemotron Nano V2 개발

## 4. [Scaling Agent Learning via Experience Synthesis](https://huggingface.co/papers/2511.03773) `meta`

LLM reinforcement learning 과정에서, 실제 데이터가 부족. 

LLM이 실제 환경 시뮬레이터처럼 동작하는 **DREAMGYM** 제안. DREAMGYM을 이용해 LLM 강화학습.



## 1. [RDMA Point-to-Point Communication for LLM Systems](https://huggingface.co/papers/2510.27656) `perplexity`

기존 RDMA solution은 하드웨어 호환성이 낮았고, 기존 collective 방식이 비효율적.

이를 해결하는, TransferEngine 라이브러리를 구현. 

low-level 논문이라서 제대로 이해하지 못했으나, 의미있는 논문으로 보여 기록만 남겼다.

:::note
RDMA: **Remote Direct Memory Access** (원격 직접 메모리 접근)

이 기술은 한 컴퓨터의 메인 메모리(RAM)에 있는 데이터를 네트워크를 통해 다른 컴퓨터의 메인 메모리로, 양쪽 컴퓨터의 운영체제(OS) 커널이나 CPU를 거치지 않고(bypass) 직접 전송하는 방식입니다.
:::