import blog_20251106_img0 from './asset/blog_20251106_img0.png';
import blog_20251106_img1 from './asset/blog_20251106_img1.png';
import blog_20251106_img2 from './asset/blog_20251106_img2.png';

# 2025-11-06 papers review

## 0. [Don't Blind Your VLA](https://huggingface.co/papers/2510.25616)
### Aligning Visual Representations for OOD Generalization

VLA 모델은 pretrained VLM의 일반화된 지식을 로봇 행동에 적용하는 것이 목표. 그러나, 제한된 로봇 행동 데이터로 VLA를 SFT하면, VLM이 원래 가졌던 시각-언어 표현이 붕괴됨. 이는 VLA가 일반화 능력을 잃고 맹목적으로 동작하게 함.

<div style={{textAlign: 'center'}}>
 <img src={blog_20251106_img2} style={{width: 500}} />
</div>

SFT Align 이라는 시각 표현 정렬 방법 제안. VLA를 cation에 대해 미세 조정하는 대신 VLA의 중간 레이어 feature를 강력하고 고정된 Vision Teacher모델의 feature와 정렬 시키는 추가적인 정규화 손실을 사용. 

## 1. [When Visualizing is the First Step to Reasoning: MIRA, a Benchmark for Visual Chain-of-Thought](https://huggingface.co/papers/2511.02779) `bytedance`

MLLM은 사고 과정이 텍스트 기반으로 이루어진다. 하지만 문제들 중에는 이미지 사고가 필요한 문제가 있음. 예를 들어서, 미로 찾기. 기어 회전.

이미지 사고 능력이 필요한 문제들과 중간 시각 자료를 포함하는 **MIRA 벤치마크** 제안. 

대부분의 SOTA MLLM 모델들이 성능이 매우 낮았으나, 중간 시각 자료를 생각 과정에 인입해주면 성능이 크게 올라감.

즉, 모델들은 시각 정보를 처리할 능력은 있으나 시각적 사고 능력은 없음을 확인.

[Latent Sketchpad](/docs/papers/y2025/oct/20251029.md#latent-sketchpad-sketching-visual-thoughts-to-elicit-multimodal-reasoning-in-mllms-microsoft) 은 마이크로소프트에서 시각적 사고 능력을 갖춘 모델을 만드려고 낸 논문이니 참조.

## 2. [When Modalities Conflict](https://huggingface.co/papers/2511.02243)
### How Unimodal Reasoning Uncertainty Governs Preference Dynamics in MLLMs

MLLM은 이미지와 텍스트 정보가 충돌할 때 어떤 modality를 따를지 결정해야 함. 기존 연구는 단순히 데이터셑 전체의 text-following ratio같은 거시적 통계로 이를 측정했음. 이 방식은 모델의 진짜 평향과 unimodal confidence가 뒤섞여 모델의 행동을 오해하게 만듬.

예를 들어서, 모델은 사실 이미지 modality를 더 선호하지만 이미지에서 뽑은 정보를 확신하지는 못할 수 있음. 이런 경우 선호도는 낮아도 확신을 강하게 하는 텍스트 oriented 정답을 내놓을 가능성이 높다.

이 논문에서는 **modality following** 행동을 **relative reasoning uncertainty**와 **inherent modality preference**라는 두 가지 핵심 요인으로 분리하여 생각함. 그 후, 시각적 난이도와 텍스트 난이도를 독립적으로 제어하는 데이터셑을 구축, 모델 출력 토큰 엔트로피로 불확실성을 정량화.

결과적으로 모든 MLLM이 최종적으로 내놓는 답변은 modal uncertainty에 의존한다고 확인. 즉, modal이 상대적으로 불확실할수록 다른 modal을 따르게 된다. 

modal uncertainty가 동일한 지점에서는 모델의 내적 선호도를 확인할 수 있는데, 대부분의 모델이 이 지점에서 내부 레이어마다 선호가 다르고 이로 인해서 불안정한 답변을 내놓았다고 함.

## 3. [THE COLLABORATION GAP](https://arxiv.org/pdf/2511.02687) `microsoft`

미래 AI 시스템은 collaboration among AI agents가 핵심. 현재 벤치마크는 AI간 협업 능력을 평가할 수 없으므로 마이크로소프트에서 제안.

두 agent에게 불완전하고 상호보완적인 미로를 제공. 소통을 해야 경로를 찾을 수 있음. 2인 협동 게임같은 걸 시켰다고 보면 될 것 같음.

**collaboration gap** 발견. 혼자서 작업을 잘 수행하는 모델이더라도 협업 환경에서는 성능이 현저히 저하됨. 심지어 homogeneous agent와의 협업도 잘 못한다. 특히 distilled model에서 이 경향성이 더 강하게 나타남.

**ordering effects** 발견. 누가 먼저 말하는지가 결과에 큰 영향을 미침. 이를 이용해서 강력한 모델이 대화 초반을 주도하는 **relay inference**를 통해 협업 성능을 개선.

## 4. [Can Visual Input Be Compressed?](https://huggingface.co/papers/2511.02650)
### A Visual Token Compression Benchmark for Large Multimodal Models

visual token pruning 방법 분석한 논문. 분석 결과, 의외로 **random token pruning**이 놀라울 정도로 강력한 베이스라인이었다고 함.

<div style={{textAlign: 'center'}}>
 <img src={blog_20251106_img0} style={{width: 500}} />
</div>


다만, OCR tawsk는 pruning에 매우 취약함.

## 5. [LTD-Bench](https://huggingface.co/papers/2511.02347) `tencent`
### Evaluating Large Language Models by Letting Them Draw 

LLM 평가시, 공간 추론 능력은 numeric metrics 가 아니라, 직접 관찰 가능한 시각적 출력을 보자. 직관적으로 해석 가능ㄹ한 시각적 결과물로 모델 평가하자.

<div style={{textAlign: 'center'}}>
 <img src={blog_20251106_img1} style={{width: 500}} />
</div>

## 6. [Reg-DPO](https://huggingface.co/papers/2511.01450) `bytedance`
### SFT-Regularized Direct Preference Optimization with GT-Pair for Improving Video Generation

