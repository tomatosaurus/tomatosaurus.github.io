import blog_20251110_img0 from './asset/blog_20251110_img0.png';
import blog_20251110_img1 from './asset/blog_20251110_img1.png';
import blog_20251110_img2 from './asset/blog_20251110_img2.png';
import blog_20251110_img2 from './asset/blog_20251110_img3.png';

# 2025-11-11 papers review

## 1. [Llama-Embed-Nemotron-8B](https://huggingface.co/papers/2511.07025) `nvidia`
### A Universal Text Embedding Model for Multilingual and Cross-Lingual Tasks

Llama-3.1 기반 Embed 모델 개발.
- uni-direction causal direction attention을 **bi-direction attention**으로 변경.
- 1600만개 쿼리-문서 쌍 데이터 활용. 이 중 절반이 합성 데이터.
- InfoNCE contrastive loss. 
- negative 학습은 mined hard negative만 사용.
- 6개의 개별 체크포인트를 average해서 사용.

https://huggingface.co/nvidia/llama-embed-nemotron-8b

## 2. [Reinforcement Learning Improves Traversal of Hierarchical Knowledge in LLMs](https://huggingface.co/papers/2511.05933) `meta`

alignment tax
- LLM 모델을 RL 튜닝시킬 때 factual memory를 잊어버리는 현상.

hyphothesis:
- 저자들은 의료코드 관련 튜닝 작업에서 오히려 성능 향상을 발견하고, 이 원인을 분석 후 alignment tax 반박.
- 관찰되는 alignment tax는 단순 암기 형태의 체계 없는 지식에서 발생하던 현상이며, 구조적이고 체계적인 지식에는 오히려 강해진다고 주장.

Experiment
- structured prompting
- path matching score
- internal activation analysis

Result
- RL은 가지고 있던 지식을 탐색하고 검색하는 procedure skill을 향상시킨다.
- 이로 인해서 좁고 구체적인 목표 달성 능력이 강해짐.
- 대신 광범위한 단순 암기 형태의 지식 처리 능력은 약해진다.

## 3. [Long Grounded Thoughts](https://huggingface.co/papers/2511.05705) `nvidia`
### Distilling Compositional Visual Reasoning Chains at Scale

multimodal reasoing 고품질 데이터셑의 부재
- 고품질 multimodal reasoning dataset 구축.

데이터 구축 프로세스
- 이미지캡션, 바운딩박스 메타 데이터를 이용해 검증 가능한 다양한 MCQ 생성.
- MCQ를 더 어려운 multi-hop Hardened MCQ로 학습.
- 질문에 대한 답변을 VLM CoT -> LLM reasoning 모델을 걸쳐 고품질 데이터로 합성.

<div style={{textAlign: 'center'}}>
 <img src={blog_20251111_img0} style={{width: 500}} />
</div>

Result: 
- 오픈 데이터 학습 비교 결과 가장 좋은 성능.

## 4. [DigiData](https://huggingface.co/papers/2511.07413) `meta`
### Training and Evaluating General-Purpose Mobile Control Agents

모바일 UI Agent 훈련 데이터셑과 훈련 방법, 평가 방법 제안.

기존 데이터셑은 application deep features를 다루지 못함. 

152k 규모의 high-quality multimodal dataset, **DigiData** 와 벤치마크 DigiData-Bench 제안.

## 5. [Do LLMs Feel?](https://huggingface.co/papers/2511.07061)
### Teaching Emotion Recognition with Prompts, Retrieval, and Curriculum Learning

근데 난 이게 철학적인 논문일줄알고 살펴봤는데, 일반적인 LLM 데이터 구축 및 학습 논문이었다.

## 6. [VADER](https://huggingface.co/papers/2511.07299) `nvidia`
### Towards Causal Video Anomaly Understanding with Relation-Aware Large Language Models

기존 Video Anomaly는 탐지에 집중. "Why"에 대한 해석이 부족.
- 이상행동의 원인이 되는 causal relationship과 interaction을 간과함.
- 이상행동은 단일 프레임이 아닌 **이전과 이후가 이어지는 맥락에서 원인과 결과**가 발생함.
- 객체 간의 정적인 관계가 아닌 동적 상호작용을 모델링 하기 어렵다.

인과적 맥락을 이해할 수 있는 모델링 기법 도입.
- **CAES(Conatext-AwarE Sampling)**: pre/on/post-event를 포착하여 핵심 프레임을 샘플링 하는 **Visual Token** 학습.
- Contrastive Relation Encoder: triplet 학습을 통해 객체간 상호작용을 의미하는 **Relation token** 학습.
- Visual tokens와 Relation tokens를 LLM에 통합하여 인과 관계를 추론.
- 이로부터 Video Anomaly에 대해 인과 관계를 포함하는 보다 깊은 이해가 가능한 LLM 구축.

Result:
- CUVA, HIVAU-70k, HAWK 벤치마크에서 SOTA혹은 compatible 달성.
- 특히 어휘적 유사성보다 인간의 판단과 유사한 MMEval, UniEval 같은 Judge 기반 지표에서 강점.
- 인과 관계 설명에서 hallucination 크게 감소.


## 7. [Reasoning with Confidence](https://huggingface.co/papers/2511.06209)
### Efficient Verification of LLM Reasoning Steps via Uncertainty Heads

일반적으로 LLM CoT confidence를 구하기 위해 PRM(Process Reward Model)을 사용한다. 
- PRM이 너무 무거움. 

UHead 제안.
- Uncertainty Quantification Heads 제안.(10M 미만)
- UHead는 모델 내부 상태를 읽어 uncertainty를 측정.
- base LLM은 frozen하고 UHead만 학습시킨다.
- 학습 데이터 생성은 PRM 방식과 비슷. 기존 벤치마크 질문에 대한 LLM 답변의 각 추론 단계를 강력한 모델이 LLM-as-a-judge로써 binary 평가.

<div style={{textAlign: 'center'}}>
 <img src={blog_20251111_img1} style={{width: 500}} />
</div>


Result:
- 9.8M 수준 UHead가 7B~8B 크기 PRM과 동등하거나 그 이상의 성능을 달성함.
- PRM이 과적합되어 성능이 저하되는 OOD 작업에서 특히 좋은 성능.

:::note
**Process Reward Model**

PRM의 최종 답변 뿐 아니라, 각 추론 과정의 각 단계를 개별적으로 평가합니다.
이를 통해 간 단계의 논리적 정합성, 목적 적합성을 판단하여 Veracity score 또는 reward를 할당.
:::


## 8. [RedOne 2.0](https://huggingface.co/papers/2511.07070)
### Rethinking Domain-specific LLM Post-Training in Social Networking Services

SNS 환경은 매우 빠르게 변화하는 환경. 
- 이로 인해 SFT 중심으로 성능을 올리면 일반 성능이 떨어지는 seesaw 현상 유발.
- 특히 소형 모델에서는 catastrophic forgetting 도 심각함.

Prposed Method:
- RL 방식을 SNS 데이터에 적용해 초기 도메인 정렬.
- SFT를 1단계에서 진단된 실패 과제를 집중적으로 학습.
  - 1단계에서 성공한 과제에 대해서는 정답 label대신, 1단계 모델의 답변으로 학습. (soft-label)
- 1,2 단계 모델의 장점만을 조합하기 위한 RL 재적용.
  - 2단계 모델에 RL 다시 적용
- Ablation study를 통해 RL->SFT->RL 이 가장 효과적이었음을 확인.

<div style={{textAlign: 'center'}}>
 <img src={blog_20251111_img2} style={{width: 500}} />
</div>

Result:
- RedOne 2.0 4B 모델이 SNS-Bench 에서 이전 SFT 중심 모델을 압도적으로 능가.
- 훨씬 거대한 모델들과도 동등한 성능을 보임.


## 9. [Generating an Image From 1,000 Words](https://huggingface.co/papers/2511.06876)
### Enhancing Text-to-Image With Structured Captions

https://bria.ai/ : 브리아AI 라는 회사에서 낸 논문. 이런저런 AI프로덕션을 하고 있는 듯.

1,000단어 이상의 prompt를 json 형태의 구조화된 캡션으로 변환시켜서 학습.

세부적인 테크닉이 들어갔지만, LLM이 긴 prompt를 이해하기 위해서 구조화가 매우 중요함을 알려주는 논문.

## 10. [Robot Learning from a Physical World Model](https://huggingface.co/papers/2511.07416) `deepmind`

<div style={{textAlign: 'center'}}>
 <img src={blog_20251111_img3} style={{width: 500}} />
</div>

비디오 생성 모델의 시각적 시연으로 물리적으로 실행 가능한 행동으로 변환할 수 있도록 하는  **PhysWorld** Framework 제안.
- 비디오는 시각적으로 그럴 듯하지만, 물리적으로 정확하지 않은 경우가 많음.

**PhysWorld**
- 비디오 생성과 World Model을 결합.
- 비디오 생성 -> 생성된 비디오 기반으로 물리적 세계 구축 -> 물리 모델 기반 로봇의 강화 학습.
  - 이 과정에서 World Model은 Object의 궤적을 보고 물리 모델을 생성함. 로봇은 이를 학습.


## 11. [DRIVE](https://huggingface.co/papers/2511.06307) `tencent`
### Data Curation Best Practices for Reinforcement Learning with Verifiable Reward in Competitive Code Generation

코드 생성 능력 향상을 위해서는 RL 학습 방법보다 데이터에 집중해야 한다고 주장.


## 12. [IterResearch](https://huggingface.co/papers/2511.07327)
### Rethinking Long-Horizon Agents via Markovian State Reconstruction

Chanllenges:
- 컨텍스트 질식 (Context Suffocation): 컨텍스트 창이 과거 기록으로 가득 차면서 새 추론을 위한 공간이 줄어듬.
- 노이즈 오염 (Noise Contamination): 초기의 탐색 오류나 관련 없는 정보가 영구적으로 남아 이후 추론을 방해합.

Proposed Method:
- 긴 작업을 MDP 로 re-define.
- 매 단계마다 워크스페이스를 전략적으로 재구성. 최선의 효율로 컨텍스트를 탐색하는 경로를 찾아감.