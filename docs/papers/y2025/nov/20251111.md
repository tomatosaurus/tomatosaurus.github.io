import blog_20251110_img0 from './asset/blog_20251110_img0.png';

# 2025-11-11 papers review

## 1. [Llama-Embed-Nemotron-8B](https://huggingface.co/papers/2511.07025) `nvidia`
### A Universal Text Embedding Model for Multilingual and Cross-Lingual Tasks

Llama-3.1 기반 Embed 모델 개발.
- uni-direction causal direction attention을 **bi-direction attention**으로 변경.
- 1600만개 쿼리-문서 쌍 데이터 활용. 이 중 절반이 합성 데이터.
- InfoNCE contrastive loss. 
- negative 학습은 mined hard negative만 사용.
- 6개의 개별 체크포인트를 average해서 사용.

https://huggingface.co/nvidia/llama-embed-nemotron-8b

## 2. [Reinforcement Learning Improves Traversal of Hierarchical Knowledge in LLMs](https://huggingface.co/papers/2511.05933) `meta`

alignment tax
- LLM 모델을 RL 튜닝시킬 때 factual memory를 잊어버리는 현상.

hyphothesis:
- 저자들은 의료코드 관련 튜닝 작업에서 오히려 성능 향상을 발견하고, 이 원인을 분석 후 alignment tax 반박.
- 관찰되는 alignment tax는 단순 암기 형태의 체계 없는 지식에서 발생하던 현상이며, 구조적이고 체계적인 지식에는 오히려 강해진다고 주장.

Experiment
- structured prompting
- path matching score
- internal activation analysis

Result
- RL은 가지고 있던 지식을 탐색하고 검색하는 procedure skill을 향상시킨다.
- 이로 인해서 좁고 구체적인 목표 달성 능력이 강해짐.
- 대신 광범위한 단순 암기 형태의 지식 처리 능력은 약해진다.

## 3. [Long Grounded Thoughts](https://huggingface.co/papers/2511.05705) `nvidia`
### Distilling Compositional Visual Reasoning Chains at Scale

multimodal reasoing 고품질 데이터셑의 부재
- 고품질 multimodal reasoning dataset 구축.

데이터 구축 프로세스
- 이미지캡션, 바운딩박스 메타 데이터를 이용해 검증 가능한 다양한 MCQ 생성.
- MCQ를 더 어려운 multi-hop Hardened MCQ로 학습.
- 질문에 대한 답변을 VLM CoT -> LLM reasoning 모델을 걸쳐 고품질 데이터로 합성.

<div style={{textAlign: 'center'}}>
 <img src={blog_20251110_img1} style={{width: 500}} />
</div>

Result: 
- 오픈 데이터 학습 비교 결과 가장 좋은 성능.