import blog_20251110_img0 from './asset/blog_20251110_img0.png';
import blog_20251110_img1 from './asset/blog_20251110_img1.png';
import blog_20251110_img2 from './asset/blog_20251110_img2.png';

# 2025-11-10 papers review

## 1. [Too Good to be Bad](https://huggingface.co/papers/2511.04962) `tencent`
### On the Failure of LLMs to Role-Play Villains

LLM 모델이 악당 역할을 잘 못한다는 논문.



## 2. [Visual Spatial Tuning](https://huggingface.co/papers/2511.05491) `bytedance`

## 3. [VERICOT](https://huggingface.co/papers/2511.04662) `amazon`
### NEURO-SYMBOLIC CHAIN-OF-THOUGHT VALIDATION VIA LOGICAL CONSISTENCY CHECKS

CoT 의 생각 과정을 1차 논리로 형식화. 각 명제가 이전 명제로부터 도출되는 전제인지 확인하여 논리적 결함을 확인하는 VERICOT 시스템 제안. 

특히 법률이나 생물 의학같은 고위험 시나리오에서 유용. VERICOT 시스템을 적용해 CoT 검증 통과율이 46% 까지 증가.

## 4. [Real-Time Reasoning Agents in Evolving Environments](https://huggingface.co/papers/2511.04898) 

real-time evolving environment 에서 시기 적절한 판단을 내려야한다. 추론이 이루어지는 동안에도 환경이 변하기 떄문에 빠른 반응속도가 필수.

그러나 빠른 반응속도 모델은 정확도가 떨어지고, 시간이 오래 걸리는 모델은 환경에 반응하지 못한다.

두 가지 방법의 장점을 합친 **Agile Thinker** 제안. 장기 계획을 세우는 Planning thread와, 단기 계획을 세우는 Agile thread를 각각 LLM으로 설계.

반응형 스레드는 계획 스레드의 추론 과정을 활용해서 빠른 시간에 비교적 나은 판단을 내린다.

Real-time reasoning이 필요한 게임 3개를 사용해 GYM을 구축. 

<div style={{textAlign: 'center'}}>
 <img src={blog_20251110_img0} style={{width: 500}} />
</div>

## 5. [Towards Mitigating Hallucinations in Large Vision-Language Models by Refining Textual Embeddings](https://huggingface.co/papers/2511.05017)

VLM의 hallucination
- 시각적 근거보다 언어적으로 그럴듯한 hallucination을 생성.
- 예를 들어서 "주방 이미지"를 보고 "과일 그릇"과 관련된 텍스트를 생성함.

문제의 원인
- 이는 VLM의 이미지 백본은 시각 정보만 활용하고 언어 백본은 텍스트로만 학습하기 때문에 발생.

VisAlign
- 전체 시각 임베딩의 average pooling 벡터를 모든 텍스트 토큰 임베딩에 concat.
- 생성된 concated 임베딩을 linear layer에 통과시켜 새로운 텍스트-이미지 임베딩 생성.
  - linear layer는 llm-backbone frozen -> end-to-end 학습 두 단계로 학습.
- 텍스트-이미지 임베딩을 사용해 LLM inference.

<div style={{textAlign: 'center'}}>
 <img src={blog_20251110_img1} style={{width: 500}} />
</div>


Result
- hallucination benchmark에서 성능 향상.
- MMVP-MLLM +9.33%, POPE-AOKVQA +2.99%, HallusionBench(Hard) +3%
- attention 분석 결과 실제로 attention이 시각 토큰와 텍스트 토큰에 고루 분포.



<div style={{textAlign: 'center'}}>
 <img src={blog_20251110_img2} style={{width: 500}} />
</div>


## 6. [CritiCal](https://huggingface.co/papers/2510.24505)
### Can Critique Help LLM Uncertainty or Confidence Calibration?

calibration의 어려움.
- LLM 모델의 안전한 사용을 위해 calibration이 중요하지만, 매우 어렵다.
- 기존 방법들은 신뢰도 점수를 뽑지만 정확하게 학습되는 방식은 없었음.
- 또한 기존 모델은 **질문의 모호함(불확실성)**과 **답변의 정확성(신뢰도)**를 구분하여 사용하지 않았음.

Critique Model
- 모델의 추론 과정과 정답 여부를 듣고 신뢰도가 높다/낮다를 판단하는 모델은 비교적 잘 동작함.
- 불확실성과 신뢰도를 평가.
  - 불확실성 : 질문의 모호함.
  - 신뢰도 : 답변이 맞을 확률.
- Critcal 모델은 모델의 질문,답변,신뢰도를 듣고 자연어 비평을 생성함.
  - 예) 답이 틀렸으므로 50%는 너무 높고 20%가 적절함"
- 스승 모델의 답변을 포함하여, 비평에 담긴 이유도 학습하도록 유도. 최종적으로 스승모델 성능을 뛰어넘었다고 함.
- '불확실성'은 개방형 질문에, '신뢰도'는 객관식 질문에 더 효과적.