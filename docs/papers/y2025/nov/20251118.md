# 2025-11-18 papers review

## 1. [Souper-Model](https://huggingface.co/papers/2511.13254) `meta`
### How Simple Arithmetic Unlocks State-of-the-Art LLM Performance

모든 과제를 잘하는 완벽한 LLM은 데이터 레시피도 매우 어렵고, 훈련 리소스도 많이 든다.
- 현재 LLM은 모델 별로 두각을 드러내는 분야가 나누어져 있음.
- SOTA 성능 모델이라고 모든 분야에서 최고 성능을 보이지는 않는다.
- 이런 모델별 강점을 무시하고 여러 모델을 앙상블하면 안됨.

**SoCE(Soup of Category Experts)**
- 분야별 전문가 모델에 더 큰 가중치를 주는 혼합 방식.
- 하위 카테고리의 벤치 마크 데이터를 이용해서 모델별 전문가 선택.

Result:
- Brekeley Function Calling Leaderboard 에서 SOTA 대비 2.7% 향상된 80.68% 성능 달성.
- 재학습없이 간단한 산술 연산 만으로 모델 성능을 끌어올렸다는 점에 의의.

## 2. [GroupRank](https://huggingface.co/papers/2511.11653)
### A Groupwise Reranking Paradigm Driven by Reinforcement Learning

RAG 시스템에서 새로운 검색 방식 제안 - **GroupRank**

- Query + Docuement Group 을 입력. 그룹 크기는 Context Window 내로 설정.
- 그룹 내의 모든 문서를 비교하여 쿼리에 대한 각 문서의 상대적 중요도 파악. **Groupwise score**
- Group 들을 병렬로 처리.
- 전체 N개 문서에 대한 글로벌 순위를 결정.

한계
- 쿼리에 대해 실제 관련성을 반영하는 점수를 부여하는 것을 목표로 함.
- 이럴거면 point-wise 로 하는 것과 뭐가 다른지.
- 결국 파이프라인 구현 논문처럼 들림.

## 3. [MMaDA-Parallel](https://huggingface.co/papers/2511.09611) `bytedance`
### Multimodal Large Diffusion Language Models for Thinking-Aware Editing and Generation



## 4. [UFO^3](https://huggingface.co/papers/2511.11332) `microsoft`
### Weaving the Digital Agent Galaxy

- https://github.com/microsoft/UFO

이종 장치간 워크 플로우 지원하는 마이크로소프트의 LLM agent!

- constellation : Galaxy에서 사용하는 DAG 개념.
- device assignment
- orchestration : 단일 오케스트레이션 

츄라이츄라이 : https://microsoft.github.io/UFO/getting_started/quick_start_galaxy/


## 5. [Genomic Next-Token Predictors are In-Context Learners](https://huggingface.co/papers/2511.12797)

**재밌다!**

일단 언어 모델에 대한 고찰. 언어 모델은 언어 이해가 아닌 언어의 통계적인 특성을 재현할 뿐이다. 이 통계적 특성이 언어라는 [논문](/docs/papers/y2025/oct/20251021.md#14-language-models-model-language)도 있고 내 의견도 그렇긴 함.

이 특성으로 인해 ICL이 동작함. 입력의 예시로부터 추상적인 패턴을 유추하고 적용할 수 있음.

ICL이 성립하기 위해 통계적 특성이 중요한 것이라면 언어가 아닌 다른 통계적 특성을 가진 시퀀스도 학습할 수 있지 않을까? 대표적으로 인간 유전자!

게놈 모델을 LLM과 유사한 규모로 학습시켜서, ICL까지 테스트해봄.

확인해본 결과 게놈 모델의 ICL 기반 패턴 유도 능력에서 **유의미한 Gain 발생** 확인함! 즉, 가설을 다시 한 번 확인. ICL은 인간 언어 이해, 언어적 특성이 아니라 통계적인 특성이 학습되고 반영된 결과일 뿐이다.

## 6. [Instella](https://huggingface.co/papers/2511.10628) `amd`
### Fully Open Language Models with Stellar Performance

AMD 에서 3B 규모의 완전 오픈된 LLM 모델 제안. 학습 데이터와 코드 베이스까지 모두 공개됨.

츄라이츄라이 : https://github.com/AMD-AGI/Instella

128k token 처리 가능한 Instella-Long, 수학 잘하는 Instella-Math도 공개.

## 7. [Dynamic Reflections](https://huggingface.co/papers/2511.02767) `deepmind`
### Probing Video Representations with Text Alignment

비디오 이해 능력은 비디오-텍스트 정렬 능력과 높은 상관 관계가 있음을 분석, 확인.