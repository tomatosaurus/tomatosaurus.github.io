import blog_20251113_img0 from './asset/blog_20251113_img0.png';
import blog_20251113_img1 from './asset/blog_20251113_img1.png';
import blog_20251113_img2 from './asset/blog_20251113_img2.png';

# 2025-11-13 papers review

## 1. [TiDAR](https://huggingface.co/papers/2511.08923) `nvidia`
###  Think in Diffusion, Talk in Autoregression 

Autoregressive & Diffusion
- Autoregressive 모델은 언어의 특징을 잘 반영하지만, 동작이 비효율적.
- Diffusion 모델은 병렬 생성이 가능하다는 장점이 있지만 AR 모델 수준 품질 달성이 어려움.
- 이로 인해서 AR과 Diffusion 모델을 섞은 연구가 종종 이루어졌왔음.

Challenges
- 그러나 기존 두 방법의 혼합 연구는 효과적으로 두 모델을 결합하지 못함.
- 주로 trade-off 에서 타협.

Proposed Method
- Attention 설계를 통해 두 모델의 장점을 결합.
- single forward pass 내에서 **Thinking** 과 **Talking**이 다른 방식으로 동작.
- **Thiking** 과정에서는 다음 과정을 위한 여러 토큰 후보를 Diffusion을 통해 미리 검토.
- **Talking** 과정에서는 Draft된 토큰들을 AR 방식으로 검증 후 샘플링.
- 그림 보는 게 조금 더 이해하기 쉬울 것.

<div style={{textAlign: 'center'}}>
 <img src={blog_20251113_img1} style={{width: 800}} />
</div>

- 입력 시퀀스를 두 부분으로 나눔. Clena prefix, Appnded Mask
- AR 파트, Clena prefix에 대해서는 Causal mask 적용.
- Diffusion 파트, 즉 Appnded Mask 에 대해서는 block 단위 bidirectional mask 적용. (여러 개의 draft가 각자의 영역 내에서 full-attention.)
- Diffusion 파트의 생성 결과 중 하나를 autoregressive 방식으로 선택해나간다. 

Result
- TiDAR은 AR 모델과의 품질 격차를 많이 해소함.
- 1.5B 모델은 품질 손상 없음. 그러나 8B 모델은 품질 손상이 있었다고 함.

Diffusion 모델과 AR 모델을 혼합하는 논문은 언제 읽어도 그냥 재밌다!

## 2. [Lumine](https://huggingface.co/papers/2511.08892) `bytedance`
### An Open Recipe for Building Generalist Agents in 3D Open Worlds

오.. 재밌어 보인다. 원신 돌렸다 ㅋㅋ  https://www.lumine-ai.org/4

<div style={{textAlign: 'center'}}>
 <img src={blog_20251113_img2} style={{width: 800}} />
</div>

일단 3D 개방형 월드의 범용 에이전트 구축을 위한 핵심 과제.
- Scalable Environment : 재현 가능한!
- Multimodal Perception
- High-level Planning
- Low-eve Control
- Memory
- Real-time inference

**Lumine**: Open Recipe, Qwen2-VL-7B-Base 모델 기반.
- 30hz 로 동작.
- pre-training -> Instruction Following -> Reasoning.
- Pre-training 단계에서는 인간 게임 플레이 데이터 학습. Instruction Following 단계에서는 지시어 태깅 데이터를 학습. 

image.png

## 3. [Motif 2 12.7B technical report](https://huggingface.co/papers/2511.07464)

LLM 모델 Motif 2 tech report.

## 4. [WebVIA](https://huggingface.co/papers/2511.06251) `tsinghua`
### A Web-based Vision-Language Agentic Framework for Interactive and Verifiable UI-to-Code Generation

UI 디자인을 코드로 자동 변환하는 제안.
- 기존 UI to code 모델들은 웹에 스크린샷을 재현하는 수준.
- 동적인 인터랙션을 거의 재현하지 못함.

<div style={{textAlign: 'center'}}>
 <img src={blog_20251113_img0} style={{width: 500}} />
</div>

Agent가 웹 상에서 실제 UI 스크린샷 촬영, 인터랙티브 도구 활용한뒤 결과를 저장한다. 그 후 각 스크린샷으로부터 코드를 재현하고 검증하는 방식으로 모델을 훈련.

UI 스크린샷의 정적 재현에서 **동적인 인터랙션 그래프까지 구현**하는 모델 제안.

## 6. [Stemming Hallucination in Language Models Using a Licensing Oracle](https://huggingface.co/papers/2511.06073)

Hallucination Challenges
- LLM 할루시네이션은 LLM이 모르는 지식이 아니라, 검증 메커니즘 없이 통계적 일관성만을 추구하는 모델과 학습의 구조적 문제라고 주장.
- Fine-tuning paradox.
- 즉, 학습 과정에서 많이 등장한 패턴이면 정답이라고 생각하도록 학습되는 것.

Challenges
- "모른다" 라고 답변하도록 학습시거나 학습 데이터를 정제하는 방식은 한계가 있다.
- RAG 시스템을 많이 사용. 그러나 "검증" 과정이 부족.

**Licensing Oracle** 제안
- 생성 프로세스에 결정론적 검증 단계(Oracle). 
- Oracle 이 답변을 출력할 라이선스를 발급. 실패하면 답변 거부.
- LLM generation -> Claim extraction -> Validation -> License 발급.
- Validation 방식은 KG와 SHACL 제약조건을 적용.

Result:
- Finuetuning 에 비해서 큰 성능 향상.
- RAG 시스템보다 성능면에서는 약간 발전, 그러나 할루시네이션을 확실히 감소시킴.

:::note
**SHACL 제약 조건**

특정 주장이 지식 그래프에 "존재" 하는지가 아니라 "유효" 한지 확인하기 위한 검증 조건들.

예를 들어서 "물체는 높은 곳에서 낮은 곳으로 흐른다." 를 제약조건으로 가지고 있다가, 강의 발원지를 하구보다 고도가 낮은 곳으로 주장하면 거부하는 방식.
:::

## 7. [WMPO](https://huggingface.co/papers/2511.09515) `bytedance`
### World Model-based Policy Optimization for Vision-Language-Action Models

이 논문 역시 World 모델 기반 VLA 강화학습 방법 제안한다.

논문의 핵심 월드 모델 활용 방법은 pixel-based video generation world model. 벡터가 아닌 픽셀 기반 비디오이기 떄문에 로봇이 생성된 이미지를 활용하기 쉬움. (로봇 역시 이미지기반으로 행동을 학습하기 때문에.)

다만, 비디오 모델은 long horizon을 생성하지 못하고 물리적 상호작용 재현에 약함. 이는 논문에서도 한계로 언급함.

[Robot Learning from a Physical World Model](/docs/papers/y2025/nov/20251111.md#10-robot-learning-from-a-physical-world-model-deepmind)이라는 물리적 한계에 집중한 논문도 있음.