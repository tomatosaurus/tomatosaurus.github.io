# oct.2025 big tech papers


## Microsoft

마이크로소프트 논문은 주제가 다양한 편. 그리고 논문들이 하나하나 엄청 개성있고 재밌다.

- [BitNet Distillation](https://huggingface.co/papers/2510.13998) (10-18)
- [DEEP SELF-EVOLVING REASONING](https://huggingface.co/papers/2510.17498) (10-21)
- [LOONGRL: REINFORCEMENT LEARNING FOR ADVANCED REASONING OVER LONG CONTEXTS](https://huggingface.co/papers/2510.19363) (10-23)
- [Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs](https://huggingface.co/papers/2510.24514) (10-29)
- [Magentic Marketplace: An Open-Source Environment for Studying Agentic Markets](https://huggingface.co/papers/2510.25779) (10-31)
- [SWIREASONING: SWITCH-THINKING IN LATENT AND EXPLICIT FOR PARETO-SUPERIOR REASONING LLMS](https://huggingface.co/papers/2510.18212) (10-07)


## NVIDIA

엔비디아 역시 주제가 꽤 다양하다. Fron-loading 논문이 개인적으로 인상깊었음.

- [VLA-0: Building State-of-the-Art VLAs with Zero Modification](https://huggingface.co/papers/2510.13054) (10-18)
- [DLER: Doing Length pEnalty Right: Incentivizing More Intelligence per Token via Reinforcement Learning](https://huggingface.co/papers/2510.15110) (10-21)
- [OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM](https://huggingface.co/papers/2510.15870) (10-21)
- [PROFBENCH MULTI-DOMAIN RUBRICS REQUIRING PROFESSIONAL KNOWLEDGE TO ANSWER AND JUDGE](https://huggingface.co/papers/2510.18941) (10-23):
- [Unified Reinforcement and Imitation Learning for Vision-Language Models](https://huggingface.co/papers/2510.19307) (10-23)
- [Long live: REAL-TIME INTERACTIVE LONG VIDEO GENERATION](https://huggingface.co/papers/2510.16917) (10-05)
- [Front-Loading Reasoning: The Synergy between Pretraining and Post-Training Data](https://huggingface.co/papers/2510.18212) (10-07)


## Google

구글 논문은 **LLM 관련 서비스 사용 경험 개선**이라는 방향성이 보이는 것 같다. 검색 품질, inference, abuse 관련 논문들.

- [LLM-GUIDED HIERARCHICAL RETRIEVAL](https://huggingface.co/papers/2510.13217) (10-18)
- [VISTA: A Test-Time Self-Improving Video Generation Agent](https://huggingface.co/papers/2510.15831) (10-21)
- [Extracting alignment data in open models](https://huggingface.co/papers/2510.18554) (10-22)
- [Soft Instruction De-escalation Defense](https://huggingface.co/papers/2510.21057) (10-27)
- [ATLAS: ADAPTIVE TRANSFER SCALING LAWS FOR MULTILINGUAL PRETRAINING, FINETUNING, AND DECODING THE CURSE OF MULTILINGUALITY](https://huggingface.co/papers/2510.22037) (10-29)
- [Supervised Reinforcement Learning: From Expert Trajectories to Step-wise Reasoning](https://huggingface.co/papers/2510.25992) (10-31)
- [Judging with Confidence: Calibrating Autoraters to Preference Distributions](https://huggingface.co/papers/2510.18672) (10-07)


## Meta

개인적으로 ray-ban meta 사용경험 관련된 논문들이라고 생각됨.

- [Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset](https://huggingface.co/papers/2510.16258) (10-21)
- [CRAG-MM: Multi-modal Multi-turn Comprehensive RAG Benchmark](https://huggingface.co/papers/2510.26160) (10-31)
- [Hybrid Architectures for Language Models: Systematic Analysis and Design Insights](https://huggingface.co/papers/2510.11288) (10-07)
- [HoneyBee: Data Recipes for Vision-Language Reasoners](https://huggingface.co/papers/2510.17269) (10-17)

## ByteDance

논문을 쏟아내는데 비디오 관련 논문의 비중이 높다. bytedance가 이미지, 비디오 관련 역량으로 world model, 더 나아가서 llm agent를 활용한 게임 산업을 생각하고 있다고 생각 중.

- [Towards General Agentic Intelligence via Environment Scaling](https://huggingface.co/papers/2509.09658) (09-22)
- [Self-Forcing++: Towards Minute-Scale High-Quality Video Generation](https://huggingface.co/papers/2510.17437) (10-04)
- [MoGA: Mixture-of-Groups Attention for End-to-End Long Video Generation](https://huggingface.co/papers/2510.18692) (10-22)
- [Grasp Any Region: Towards Precise, Contextual Pixel Understanding for Multimodal LLMs](https://huggingface.co/papers/2510.18876) (10-22)
- [Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal Evidence](https://huggingface.co/papers/2510.20579) (10-24)
- [Video-As-Prompt: Unified Semantic Control for Video Generation](https://huggingface.co/papers/2510.20888) (10-27)
- [Game-TARS: Pretrained Foundation Models for Scalable Generalist Multimodal Game Agents](https://huggingface.co/papers/2510.23691) (10-29)
- [From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors](https://huggingface.co/papers/2510.17439) (10-29)
- [Scaling Latent Reasoning via Looped Language Models](https://huggingface.co/papers/2510.25741) (10-30)
- [Parallel Loop Transformer: Efficient Test-Time Computation Scaling](https://huggingface.co/papers/2510.24824) (10-30)
- [PairUni: Pairwise Training for Unified Multimodal Language Models](https://huggingface.co/papers/2510.25682) (10-30)
- [SAIL-Embedding: Omni-modal Embedding Foundation Model](https://huggingface.co/papers/2510.18245) (10-17)


## Adobe

이미지, 비디오 편집 관련 논문.

- [PI-FLOW: POLICY-BASED FEW-STEP GENERATION VIA IMITATION DISTILLATION](https://huggingface.co/papers/2510.14974) (10-18)
- [UniFusion: Vision-Language Model as Unified Encoder in Image Generation](https://huggingface.co/papers/2510.17790) (10-17)

## Apple

- [UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action](https://huggingface.co/papers/2510.17790) (10-21)
- [Pico-Banana-400K: A Large-Scale Dataset for Text-Guided Image Editing](https://huggingface.co/papers/2510.19808) (10-23)
- [AToken: A Unified Tokenizer for Vision](https://huggingface.co/papers/2509.09658) (09-22)
- [DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search](https://huggingface.co/papers/2510.18554) (10-16)


## Alibaba

- [Winning the Pruning Gamble: A Unified Approach to Joint Sample and Token Pruning for Efficient Supervised Fine-Tuning](https://huggingface.co/papers/2510.16917) (10-05)

## Amazon

- [DISTRACTOR INJECTION ATTACKS ON LARGE REASONING MODELS: CHARACTERIZATION AND DEFENSE](https://huggingface.co/papers/2510.16259) (10-21)
- [SCALING LAWS MEET MODEL ARCHITECTURE: TOWARD INFERENCE-EFFICIENT LLMS](https://huggingface.co/papers/2510.18245) (10-27)

## Tencent
- [LaSeR: Reinforcement Learning with Last-Token Self-Rewarding](https://huggingface.co/papers/2510.14943) (10-18)
- [Every Question Has Its Own Value: Reinforcement Learning with Explicit Human Values](https://huggingface.co/papers/2510.20187) (10-24)
- [ChronoPlay: A Framework for Modeling Dual Dynamics and Authenticity in Game RAG Benchmarks](https://huggingface.co/papers/2510.18455) (10-30)

## BAAI
- [Emu3.5: Native Multimodal Models are World Learners](https://huggingface.co/papers/2510.25992) (10-31)

## Huggingface
- [FineVision: Open Data Is All You Need](https://huggingface.co/papers/2510.17269) (10-21)


## Qualcomm
- [VIDEO REASONING WITHOUT TRAINING](https://huggingface.co/papers/2510.17045) (10-22)

## Salesforce
- [Enterprise Deep Research: Steerable MultiAgent Deep Research for Enterprise Analytics](https://huggingface.co/papers/2510.17797) (10-21)

## Xiaomi
- [Stabilizing MoE Reinforcement Learning: Aligning Training and Inference Routers](https://huggingface.co/papers/2510.11370) (10-27)
