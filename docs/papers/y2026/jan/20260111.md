import blog_20260111_img0 from './asset/blog_20260111_img0.png';

# 2026-01-01 papers review

happy new year. 

## 1. [GDPO](https://huggingface.co/papers/2601.05242) `nvidia`
### Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization

최근 LLM이 많이 채용하는 multi-reward 학습에서도 GRPO를 활용함.

GRPO 방식의 목적은 다양한 답변들을 생성해서 답변들간의 비교를 통해 더 좋은 정답을 찾는 방식이다. 

multi-reward 방식은 이 과정에서 여러 가지의 평가 기준(그룹) 을 사용해서 각 답변을 평가합니다. GRPO는 하나의 답변에 대해 각각의 평가 기준을 얼마나 잘 만족시켰는지 합산하여 평균을 내고, 이 평균들을 취합하여 Advantage를 계산합니다. 

GDPO 방식은 반면 목표 별로 답변들의 평균 점수를 먼저 계산하고, 이 목표별 점수를 취합하여 Advantage를 계산합니다. 이 떄 상대적으로 취약한 목표의 가중치가 최종 Advantage 계산에 더 많이 반영됩니다.

이로 인해서 GDPO 방식은 모델이 얼마나 특정 목표를 잘 달성하는지를 GRPO 방식보다 더 명확하게 평가할 수 있습니다.

개발자는 GRPO 코드를 간단하게 교체하는 것만으로 GDPO를 사용할 수 있다고 코멘트 남겼네요.

```
GDPO is a drop-in replacement for GRPO in verl and TRL — only minor code changes needed.

We release a slurm-free, easy-to-run implementation supporting multiple RL frameworks (verl / TRL / NeMo-RL) so you can quickly validate GDPO on tool-calling and math reasoning tasks.

⏱️ Each run can be completed in ~1 hour on 8×A100s, or ~2.5 hours on a single A100.

🔄 Switching from GRPO to GDPO is easy.
👉 Try it yourself: https://github.com/NVlabs/GDPO
```

## 2. [Entropy-Adaptive Fine-Tuning](https://ymxyll.github.io/EAFT/)
### Resolving Confident Conflicts to Mitigate Forgetting

SFT 에서 catastrophic forgetting을 줄이기 위한 논문. 비슷한 논문을 많이 본 것 같다.

학습 과정에서 모델 토큰 생성시 모델의 엔트로피가 낮은데도 불구하고 확률이 낮은 정답 토큰이 확인되는 경우에는, 해당 토큰의 가중치를 낮춰서 학습.

저런 경우에는 모델이 가지고 있는 내부 신념과 다른 값, 즉 학습 데이터의 오류일 확률이 높다. 그런데 중요한 문제는 저런 경우가 cross-entropy 구조 상 gradient 에 매우 큰 영향을 미치게 된다는 것.

SFT는 갈수록 LLM 모델 자체는 신뢰하고 답변의 형식만을 맞춰주는 방향으로 나갈 것 같다.

## 3. [Youtu-Agent](https://huggingface.co/papers/2512.24615)
### Scaling Agent Productivity with Automated Generation and Hybrid Policy Optimization

테크 대기업들의 Agent 개발 경쟁이 치열하다. 더불어 Agent orchestration tool의 개발 경쟁 역시 매우 치열하다. 좋은 Agent orchestration tool 이 open source 로 풀리기도 한다. 

AI Agent 초기 세팅과 미세 튜닝의 복잡성에 대해 이야기 하면서 실행 환경, 툴킷, 컨텍스트 관리를 분리(Decoupling)하여 유연한 재사용과 자동 합성이 가능하도록 설계. 심지어 도구까지 자동화함.


<div style={{textAlign: 'center'}}>
 <img src={blog_20260111_img0} style={{width: 500}} />
</div>

1단계: 에이전트 자동 생성 (Generation)
사용자가 "나는 이런 일을 하는 비서가 필요해"라고 요구사항을 입력하면 시스템이 자동으로 설계를 시작합니다.

Workflow 모드: 이미 정해진 순서가 있는 일(예: 매일 아침 뉴스 요약)이라면, 표준화된 절차에 맞춰 도구와 프롬프트를 조립합니다.

Meta-Agent 모드: 복잡하고 처음 해보는 일이라면, **'메타 에이전트'**라는 상위 AI가 등판합니다. 이 AI는 필요한 도구(Python 코드 등)를 직접 코딩하고, 에이전트의 성격(프롬프트)과 설정을 스스로 생성합니다.

특징: 사람이 일일이 코딩하고 프롬프트를 깎을 필요가 없습니다.

2단계: 에이전트의 실전 연습 (Agent Practice)
에이전트가 만들어지자마자 완벽할 수는 없습니다. 그래서 배포된 후 실전 경험을 쌓으며 성능을 높입니다.

In-context Optimization: 일을 하다가 실패하거나 성공한 경험을 '메모리'에 저장합니다.

피드백 루프: 다음번에 비슷한 일을 할 때, 과거의 성공 사례를 참고하여 자신의 프롬프트를 스스로 수정(Self-correction)합니다.

장점: 모델의 뇌(파라미터)를 직접 건드리지 않고도 경험치만으로 즉각 성능이 좋아집니다.

3단계: 하이브리드 정책 최적화 (Hybrid Optimization)
에이전트가 어느 정도 경험 데이터를 쌓으면, 이제 본격적으로 모델 자체를 똑똑하게 만듭니다.

Agent RL (강화학습): '실전 연습' 단계에서 모인 수많은 성공/실패 데이터를 바탕으로 강화학습을 진행합니다.

하이브리드: "메모리를 활용한 잔기술(Practice)"과 "모델 자체의 지능(RL)"을 모두 강화하여, 어떤 환경에서도 최적의 성능을 내도록 모델을 고도화합니다.

## 4. [Can LLMs Predict Their Own Failures? Self-Awareness via Internal Circuits](https://huggingface.co/papers/2512.20578)

LLM 모델 내부 회로의 신호를 탐지하여 LLM 모델이 현재 올바른 정답을 말하는지 hallucination을 일으키고 있는지 탐지하는 경량 모델 Gnosis 를 제안.

마치 인간의 생체 신호를 탐지해 거짓말 여부를 파악하는 거짓말 탐지기의 기계 버전이라고 봐도 될듯.

hidden state 와 Attentino pattern 을 파악함. hidden state 가 이상하다거나, Attention pattern 이 정답과 상관없거나 정보간의 연결이 느슨해지는 패턴이라던가.

답변이 절반 정도만 생성되어도 이 수치를 통해 답변의 성패를 파악할 수 있음.

https://github.com/Amirhosein-gh98/Gnosis
