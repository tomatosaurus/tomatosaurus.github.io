---
id: rl_elemetns
sidebar_position: 1
title: Elements
---

# Elements of Reinforcmenet Learning

**보상 극대화** 라는 단어를 Reinforcement Learning에서는 아래와 같이 규정한다.

체스게임을 생각해보자. 플레이어가 있고, 상대 플레이어가 있다. 체스판과 체스말도 있다. 매 차례 플레이어는 체스판과 체스말, 상대 플레이어 그리고 그외의 모든 것을 마주한다. 물론 가장 중요한 건 현재 체스판의 상태일 것이다. 그리고 플레이어는 모든 것을 종합적으로 고려해 이기기 위한 최선의 선택을 하려할 것이다. 

**Agent**: 플레이어. 학습하는 주체이다. Agent는 상황을 고려해서 행동한다. 이 때 주어진 상황에 다른 agent 의 행동을 강화 학습에는 확률 분포로 모델링한다. 이를 **Policy**라고 한다.

**Action**: 플레이어가 상황에 반응해서 취하는 상호작용.

**policy**: 주어진 상황에서 Agent 가 어떤 행동을 할 확률. $\pi(a|s)$로 기술한다.

**Environment**: Agent 를 제외한 모든 것.

**state**: envrionmnet의 상태.

**Reward**: 행동에 의해 주어지는 보상. 강화 학습에서 Reward라는 단어는 즉각적인 보상만을 의미한다. 예를 들어서, 체스 게임에서 퀸을 움직여서 폰을 잡았다. 폰을 잡은 것을 +1 이라는 숫자로 수치화했다고 하자. 이번 차례 플레이어 행동의 Reward 는 +1 이라고 설명할 수 있다. 하지만 이 action은 좋은 행동이었을까?

몇 턴후, 무리하게 상대 진영으로 들어간 내 퀸이 죽었다. 퀸은 10점짜리 기물이므로 해당 턴 플레이어가 받은 Reward는 -10점이다. 몇 턴전의 무리한 움직임으로 인한 손해다.

강화학습에서는 그래서 시나리오가 끝날 떄까지의 총보상이 가장 중요하다. 물론 인생과 같은 시나리오는 너무 길어서 예상할 수 없기에 시야를 줄이기도 하지만 어쩄든 중요한 것은 **가능한 한 장기적인 관점에서의 총보상** 을 생각하는 것이다. 이를 강화학습에서는 **Value Function**이라고 한다.

**Value Function**: 장기적으로 얻을 수 있는 총 보상의 **기댓값**. 기댓값이라는 단어를 유념하자. 강화학습에서는 기댓값, 예측값 그리고 가정이 많으므로 이 부분을 꼼꼼히 살펴야 한다.

그럼 가장 첫 줄의 **보상 극대화** 라는 단어가 조금 더 명확하게 설명이 된다.

::note
**강화학습의 목표**

주어진 state에서 agent가 장기적으로 기대할 수 있는 총보상을 최대화하는 policy를 학습하는 것.

== 주어진 state에서 agent가 장기적인 총보상의 기댓값을 최대화하는 행동을 선택하도록 하는 것!
:::

## Associative, Non-associative

그럼 이 목표를 어떻게 달성할 수 있을까? 

고려해야할 게 많지만 일단은 가장 간단한 시나리오를 먼저 생각해보자. State가 바뀌지 않는 상황이다.

어떤 행동을 하더라도 State는 항상 동일하다. 룰렛 머신과 같은 상황이다. 룰렛 머신은 몇 번을 돌리더라도 이상적으로는 항상 같은 상태가 되어 내 행동을 기다린다. 이런 시나리오를 **Non-associative Scenario**라고 한다. 우리는 먼저 Non-associative scenario에서 최적의 행동을 선택하는 방법을 알아볼 것이다.

하지만 현실에서 non-associative scenario는 그렇게 많지 않다. 모든 상황은 우리의 선택에 따라 변화한다. 선택에 따라 모든 순간 서로 다른 state 를 마주해야 하는 시나리오를 **associative scenario**라고 한다. 이에 대해서는 추후에 다루려 한다.

## Stationary, Non-stationary

중요한 개념이지만, 이 블로그에서는 일단 간단하게만 언급하려고 한다. LLM을 위한 강화 학습 공부를 위한 기초 지식을 쌓고자 하는 목적에 비해 서술이 너무 길어질까하는 걱정떄문이다.

**stationary problem**은 environment 가 안정적인 확률 분포에 따라 행동에 반응하는 상황이다. 언제나 같은 잭팟 확률을 나타내는 룰렛 머신이 예시이다. 대부분의 디지털 시스템도 좋은 예시다.

**non-stationary problem**은 반대의 상황이다. 매번 확률이 변동되는 룰렛 기계. 알 수 없이 반응하는 자연이나 성격이상자들이 좋은 예시이다.

이 글들에서는 stationary problem을 기본적으로 상정할 것이고, non-stationary에 대해서 이야기할때는 명확히 앞서 언급할 것이다. 