---
id: rl_associative_mdp
sidebar_position: 2
title: Associative, MDP
---

# Associative, Environment-aware
## Markov Decision Processes

이번에는 매 시간마다 환경이 변하는 상황을 생각해보자. 조금 더 문제를 단순화해서 모든 환경 변화는 agent의 선택에 의해 결정된다고 해보자. 

집을 나와서 회사까지 가는 일를 생각해보자. 나(Agent)의 목적은 이동 시간을 최대한 적게 소모하는 것이다. 여러가지 선택지가 생긴다. 집을 나와서 버스, 지하철, 택시를 선택할 수 있다. 걸어가는 길에서도 여러가지 갈림길이나 다른 경로를 선택할 수 있을 것이다. 환승도 생각해볼 수 있고 아무튼 조금 생각해보면 정말 경우의 수가 다양하다. 

아마도 대부분의 사람은 회사에 입사한 초기에는 지도를 켜서 몇 가지 경로를 사용해보다가 시간이 지날수록 한가지 경로로 정착하게 될것이다. 당신은 왜그렇게 정착했는가? 당신이 선택한 경로가 언제나 최선의 경로였나?

## Environmnet-aware

출근을 오래하면 아마 많은 데이터가 쌓일 것이다. 어떤 길이 평소에 얼마나 막히는지, 사람이 얼마나 많거나 다니기에 불편한 길도 있을 것이다. 길이 막히면 얼마나 걸리고 길이 뚫리면 시간이 얼마나 걸리는지도 알 수 있다. 또, 공사같이 잘 일어나지 않는 일도 경험해봤을 것이고 비가 오는 날이나 눈이 오는 날에는 어떤 길에 어떤 일이 일어날수 있는지 어느정도 알 고 있을 것이다. 이를 강화 학습에서는 **Environment-aware**이라고 한다. Agent는 어떤 상황에서 어떤 선택을하면 시간(보상)이 얼마나 걸리고 무슨 상황에 처하게 될지 확률을 알고 있다. 집(state)에서 출발할 차를 타기(action)로 한다면 30% 확률(probability)로 차가 막혀서 10분(reward)을 소모해 고속도로(state')에 올라가야할 수 있다. 50% 확률로 차가 별로 악 막혀서 30분(reward)을 소모해 회사 근처(state')까지 쭉 갈수도 있을 것이다. 

이렇게 (상황, 행동) 에 따른 보상과 다음 상황의 확률을 알고 있는 것을 **environment-award**이라고 한다. 보다 수학적으로는 environment는 아래와 같이 기술되며, dynamics 라고 부른다.

$$
p(s', r| s, a)
$$

dynamics를 알고 있을 떄 이론적으로 무조건 최선의 선택을 할 수 있을 것 같다. 당신이 매일 아침 날씨와 뉴스를 고려해서 출근길을 바로바로 선택하듯이. 지금부터 당신과 같은, 또는 당신보다 나은 선택을 할 수 있는 agent를 어떻게 만들 수 있는지 알아보자.

## Policy

기본적으로 Agent를 만든다는 것은 상황이 주어졌을 떄 어떤 행동을할 확률을 모델링한다는 것이다. 주어진 상황에서 행동의 확률 분포를 **policy**라고 하며 $\pi(a|s)$라고 한다.

## Bellman Equation

시간 $t$의 state $S_t$에서 $S_{t+1}$로 갈 때 획득하는 보상을 $R_{t+1}$이라고 하자.

$S_t, S_{t+1}, ... , S_T$ 라는 시나리오가 주어졌을 때 획득가능한 총 보상은 아래와 같이 정의할 수 있다.

$$
G_t = R_{t+1} + R_{t+2} + .. R_{T-1} + R_{T}
$$

강화 학습에서는 최근에 획득한 보상일수록 높은 weight를 부여하기도 한다. 이 경우에는 아래와 같이 정의한다. (좀 더 일반적인 형태의 식이다.)

$$
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots + \gamma^{T-t-1} R_T = \sum_{k=0}^{T-t-1} \gamma^k R_{t+k+1}$$
$$

state가 주어졌을 떄 총 보상의 기댓값은 **state-value function**이라고 하며, 아래와 같이 쓸 수 있다. 

$$
v_{\pi}(s) = E_{\pi}[G_t|S_t=s]
$$

state와 action이 주어졌을 때 총 보상의 기댓값은 **state-action value function**이라고 하며 아래와 같이 쓸 수 있다. 

$$
q_{\pi}(s, a) = E_{\pi}[G_t|S_t=s, A_t=a]
$$