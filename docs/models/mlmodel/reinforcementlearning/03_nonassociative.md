---
id: rl_nonstationary
sidebar_position: 2
title: Non-associative
---

# Non-associative Scenario

Non-associative Scenario의 대표적이고 가장 잘 알려진 예시는 **슬롯머신**이다. 슬롯머신이 하나 뿐이면 행동의 선택 여지가 없으니 여러 대의 슬롯머신이 준비된 방을 생각해보자. 여러분에게 주어진 정보는 각 슬롯머신이 서로 다른 확률로 잭팟이 나온다는 것 뿐이다. 자, 어떻게 해야 최대한 돈을 많이 벌어서 방을 나갈 수 있을까?

먼저 **Non-associative Scenario**가 무엇인지 한 번만 더 이야기하고 지나가자. Non-associative scenario는 Agent의 행동에도 상황이 바뀌지 않는 시나리오다. Agent는 모든 time에 똑같은 상황에 처한다. 그러므로 Agent는 장기적인 총보상을 극대화하기 위해서는, 현재 상황의 보상을 극대화해야 한다. 

어떻게 보상을 최대화할 수 있을까? 모든 머신러닝이 그렇지만, 우리가 처한 상황을 조금 더 명확하게 정리해보자.

## Condition

- 환경, environment의 변화가 없다. state가 동일하다.
- agent는 초기에느 action에 따른 reward 를 알 수 없다.
- agent는 action이후에 reward를 확인할 수 있다.
- action에 따른 reward는 고정이 아니다. stochastic하게 주어진다.

## What Agent Do?

Agent에게 최선의 선택은 보상 기댓값이 가장 높은 슬롯머신을 플레이하는 것이다. 슬롯머신 선택(action)에 다른 보상(reward)의 기댓값$E[R_t|A_t=a]$을 최대화하는 것이다. 다시 한번  말하지만 non-associative 시나리오에서 총보상을 최대화하는 것은, 현재의 보상을 극대화하는 것과 같은 의미이다. 그러므로 $E[R_t|A_t=a]$가 행동의 총보상과 같은 의미를 가진다고 할 수 있는데, 여기서 **행동에 따른 총보상**을 $q(a)$라고 한다. agent는 알 수 없지만 총보상은 실제로는 아래와 같다. 

$$
q_{*}(a)=E[R_t|A_t=a]
$$

언급했듯 **Agent는 $q_*(a)$를 알 수 없다.** 그러므로 non-associative scenario에서 agent의 최우선 과제는 $q_{*}$를 짐작(estimate)하는 것이다. 

앞으로 강화학습 글에서 $q_{*}$의 측정값을 $Q_t(a)$라고 정의할것이다. $Q_t(a)$를 어떻게 알 수 있을까?

어려운 방법을 고민할 필요 없다. 가장 효과적인 방법 중 하나는 여러가지 슬롯머신을 각각 여러번 돌려본 뒤 보상의 평균을 계산하는 방식이다.

## Exploit-Exploration

강화학습에서 가장 어려운 문제는 **Exploit-Exploration** 문제다. exploit은 현재까지 알고 있는 최선의 선택을 하는 것이고, explore는 새로운 선택지를 탐험해보는 것이다.

다음과 같은 상황을 생각해보자. 당신은 10개의 슬롯머신을 차례대로 10번씩 실행해보고 있다. 1~5번까지의 슬롯머신에서는 0~2번 정도의 잭팟이 터졌는데 6번 슬롯머신에서 갑자기 10번 중 8번의 잭팟이 터졌다!  당신은 7~10번 슬롯머신을 테스트해볼까, 아니면 남은 모든 기회를 6번 슬롯머신에 투자할까?

이 지점의 딜레마를 **exploit-exploration problem**이라고 한다. 6번 슬롯머신을 계속 돌릴 것인가 새로운 슬롯머신을 테스트해볼 것인가? 어쩌면 10번 슬롯머신이 확률 100%의 잭팟 기계일수도 있다. 이 문제는 상당히 어려운 문제이며 명확한 해답이 없다. 지금까지도 강화학습의 가장 중요한 고려사항이다. 일반적인 슬롯머신에 대한 이해가 있다면 10번 중 8번의 확률은 말도 안되게 높은 확률이므로 더이상 다른 기계를 테스트해보지 않을 것이다. 시행횟수도 중요하다. 더이상 시행횟수가 충분히 남아 있지 않다면 다른 슬롯머신을 테스트해볼수도없다. 강화학습에서는 고려할 수 있는 모든 것을 고려하여 가장 기대값이 높으 타협점을 찾고자 한다.

가장 대중적이고 편하게 사용할 수 있는 방법은 $\eps -random$ 방식이다. 높은 확률로 현재까지 탐색 결과 중 최선을 선택하고 낮은 확률로 랜덤으로 선택지를 고르는 것이다. 후반으로 갈수록 $\eps$ 값을 낮춘는 등의 다양한 variation을 테스트해볼 수 도 있다.

**exploit-exploration**의 타협점을 찾는 것은 상황에 따라 다르고 너무 변칙적이고 세세하므로 이 글에서는 $\eps -random$외의 방법을 논의하지 않는다.
