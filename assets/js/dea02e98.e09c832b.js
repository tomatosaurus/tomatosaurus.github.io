"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[3097],{8453:(e,n,i)=>{i.d(n,{R:()=>a,x:()=>o});var r=i(6540);const t={},s=r.createContext(t);function a(e){const n=r.useContext(s);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:a(e.components),r.createElement(s.Provider,{value:n},e.children)}},8707:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>h,contentTitle:()=>d,default:()=>u,frontMatter:()=>c,metadata:()=>r,toc:()=>g});const r=JSON.parse('{"id":"papers/y2025/nov/20251106","title":"2025-11-06 papers review","description":"0. Don\'t Blind Your VLA","source":"@site/docs/papers/y2025/nov/20251106.md","sourceDirName":"papers/y2025/nov","slug":"/papers/y2025/nov/20251106","permalink":"/docs/papers/y2025/nov/20251106","draft":false,"unlisted":false,"editUrl":"https://github.com/logicbaron/logicbaron.github.io/tree/dev/docs/papers/y2025/nov/20251106.md","tags":[],"version":"current","frontMatter":{},"sidebar":"Y2025Sidebar","previous":{"title":"2025-11-04 papers review","permalink":"/docs/papers/y2025/nov/20251104"},"next":{"title":"2025-11-07 papers review","permalink":"/docs/papers/y2025/nov/20251107"}}');var t=i(4848),s=i(8453);const a=i.p+"assets/images/blog_20251106_img0-8c3baf1d49fa78106b0a2f71091ce01f.png",o=i.p+"assets/images/blog_20251106_img1-92c0f76c80000b61d4cee54938a5f0d2.png",l=i.p+"assets/images/blog_20251106_img2-ddc673497053d4d142f15ea432a7806c.png",c={},d="2025-11-06 papers review",h={},g=[{value:"0. Don&#39;t Blind Your VLA",id:"0-dont-blind-your-vla",level:2},{value:"Aligning Visual Representations for OOD Generalization",id:"aligning-visual-representations-for-ood-generalization",level:3},{value:"1. When Visualizing is the First Step to Reasoning: MIRA, a Benchmark for Visual Chain-of-Thought <code>bytedance</code>",id:"1-when-visualizing-is-the-first-step-to-reasoning-mira-a-benchmark-for-visual-chain-of-thought-bytedance",level:2},{value:"2. When Modalities Conflict",id:"2-when-modalities-conflict",level:2},{value:"How Unimodal Reasoning Uncertainty Governs Preference Dynamics in MLLMs",id:"how-unimodal-reasoning-uncertainty-governs-preference-dynamics-in-mllms",level:3},{value:"3. THE COLLABORATION GAP <code>microsoft</code>",id:"3-the-collaboration-gap-microsoft",level:2},{value:"4. Can Visual Input Be Compressed?",id:"4-can-visual-input-be-compressed",level:2},{value:"A Visual Token Compression Benchmark for Large Multimodal Models",id:"a-visual-token-compression-benchmark-for-large-multimodal-models",level:3},{value:"5. LTD-Bench <code>tencent</code>",id:"5-ltd-bench-tencent",level:2},{value:"Evaluating Large Language Models by Letting Them Draw",id:"evaluating-large-language-models-by-letting-them-draw",level:3},{value:"6. Reg-DPO <code>bytedance</code>",id:"6-reg-dpo-bytedance",level:2},{value:"SFT-Regularized Direct Preference Optimization with GT-Pair for Improving Video Generation",id:"sft-regularized-direct-preference-optimization-with-gt-pair-for-improving-video-generation",level:3}];function p(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",p:"p",strong:"strong",...(0,s.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"2025-11-06-papers-review",children:"2025-11-06 papers review"})}),"\n",(0,t.jsxs)(n.h2,{id:"0-dont-blind-your-vla",children:["0. ",(0,t.jsx)(n.a,{href:"https://huggingface.co/papers/2510.25616",children:"Don't Blind Your VLA"})]}),"\n",(0,t.jsx)(n.h3,{id:"aligning-visual-representations-for-ood-generalization",children:"Aligning Visual Representations for OOD Generalization"}),"\n",(0,t.jsx)(n.p,{children:"VLA \ubaa8\ub378\uc740 pretrained VLM\uc758 \uc77c\ubc18\ud654\ub41c \uc9c0\uc2dd\uc744 \ub85c\ubd07 \ud589\ub3d9\uc5d0 \uc801\uc6a9\ud558\ub294 \uac83\uc774 \ubaa9\ud45c. \uadf8\ub7ec\ub098, \uc81c\ud55c\ub41c \ub85c\ubd07 \ud589\ub3d9 \ub370\uc774\ud130\ub85c VLA\ub97c SFT\ud558\uba74, VLM\uc774 \uc6d0\ub798 \uac00\uc84c\ub358 \uc2dc\uac01-\uc5b8\uc5b4 \ud45c\ud604\uc774 \ubd95\uad34\ub428. \uc774\ub294 VLA\uac00 \uc77c\ubc18\ud654 \ub2a5\ub825\uc744 \uc783\uace0 \ub9f9\ubaa9\uc801\uc73c\ub85c \ub3d9\uc791\ud558\uac8c \ud568."}),"\n",(0,t.jsx)("div",{style:{textAlign:"center"},children:(0,t.jsx)("img",{src:l,style:{width:500}})}),"\n",(0,t.jsx)(n.p,{children:"SFT Align \uc774\ub77c\ub294 \uc2dc\uac01 \ud45c\ud604 \uc815\ub82c \ubc29\ubc95 \uc81c\uc548. VLA\ub97c cation\uc5d0 \ub300\ud574 \ubbf8\uc138 \uc870\uc815\ud558\ub294 \ub300\uc2e0 VLA\uc758 \uc911\uac04 \ub808\uc774\uc5b4 feature\ub97c \uac15\ub825\ud558\uace0 \uace0\uc815\ub41c Vision Teacher\ubaa8\ub378\uc758 feature\uc640 \uc815\ub82c \uc2dc\ud0a4\ub294 \ucd94\uac00\uc801\uc778 \uc815\uaddc\ud654 \uc190\uc2e4\uc744 \uc0ac\uc6a9."}),"\n",(0,t.jsxs)(n.h2,{id:"1-when-visualizing-is-the-first-step-to-reasoning-mira-a-benchmark-for-visual-chain-of-thought-bytedance",children:["1. ",(0,t.jsx)(n.a,{href:"https://huggingface.co/papers/2511.02779",children:"When Visualizing is the First Step to Reasoning: MIRA, a Benchmark for Visual Chain-of-Thought"})," ",(0,t.jsx)(n.code,{children:"bytedance"})]}),"\n",(0,t.jsx)(n.p,{children:"MLLM\uc740 \uc0ac\uace0 \uacfc\uc815\uc774 \ud14d\uc2a4\ud2b8 \uae30\ubc18\uc73c\ub85c \uc774\ub8e8\uc5b4\uc9c4\ub2e4. \ud558\uc9c0\ub9cc \ubb38\uc81c\ub4e4 \uc911\uc5d0\ub294 \uc774\ubbf8\uc9c0 \uc0ac\uace0\uac00 \ud544\uc694\ud55c \ubb38\uc81c\uac00 \uc788\uc74c. \uc608\ub97c \ub4e4\uc5b4\uc11c, \ubbf8\ub85c \ucc3e\uae30. \uae30\uc5b4 \ud68c\uc804."}),"\n",(0,t.jsxs)(n.p,{children:["\uc774\ubbf8\uc9c0 \uc0ac\uace0 \ub2a5\ub825\uc774 \ud544\uc694\ud55c \ubb38\uc81c\ub4e4\uacfc \uc911\uac04 \uc2dc\uac01 \uc790\ub8cc\ub97c \ud3ec\ud568\ud558\ub294 ",(0,t.jsx)(n.strong,{children:"MIRA \ubca4\uce58\ub9c8\ud06c"})," \uc81c\uc548."]}),"\n",(0,t.jsx)(n.p,{children:"\ub300\ubd80\ubd84\uc758 SOTA MLLM \ubaa8\ub378\ub4e4\uc774 \uc131\ub2a5\uc774 \ub9e4\uc6b0 \ub0ae\uc558\uc73c\ub098, \uc911\uac04 \uc2dc\uac01 \uc790\ub8cc\ub97c \uc0dd\uac01 \uacfc\uc815\uc5d0 \uc778\uc785\ud574\uc8fc\uba74 \uc131\ub2a5\uc774 \ud06c\uac8c \uc62c\ub77c\uac10."}),"\n",(0,t.jsx)(n.p,{children:"\uc989, \ubaa8\ub378\ub4e4\uc740 \uc2dc\uac01 \uc815\ubcf4\ub97c \ucc98\ub9ac\ud560 \ub2a5\ub825\uc740 \uc788\uc73c\ub098 \uc2dc\uac01\uc801 \uc0ac\uace0 \ub2a5\ub825\uc740 \uc5c6\uc74c\uc744 \ud655\uc778."}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.a,{href:"/docs/papers/y2025/oct/20251029#latent-sketchpad-sketching-visual-thoughts-to-elicit-multimodal-reasoning-in-mllms-microsoft",children:"Latent Sketchpad"})," \uc740 \ub9c8\uc774\ud06c\ub85c\uc18c\ud504\ud2b8\uc5d0\uc11c \uc2dc\uac01\uc801 \uc0ac\uace0 \ub2a5\ub825\uc744 \uac16\ucd98 \ubaa8\ub378\uc744 \ub9cc\ub4dc\ub824\uace0 \ub0b8 \ub17c\ubb38\uc774\ub2c8 \ucc38\uc870."]}),"\n",(0,t.jsxs)(n.h2,{id:"2-when-modalities-conflict",children:["2. ",(0,t.jsx)(n.a,{href:"https://huggingface.co/papers/2511.02243",children:"When Modalities Conflict"})]}),"\n",(0,t.jsx)(n.h3,{id:"how-unimodal-reasoning-uncertainty-governs-preference-dynamics-in-mllms",children:"How Unimodal Reasoning Uncertainty Governs Preference Dynamics in MLLMs"}),"\n",(0,t.jsx)(n.p,{children:"MLLM\uc740 \uc774\ubbf8\uc9c0\uc640 \ud14d\uc2a4\ud2b8 \uc815\ubcf4\uac00 \ucda9\ub3cc\ud560 \ub54c \uc5b4\ub5a4 modality\ub97c \ub530\ub97c\uc9c0 \uacb0\uc815\ud574\uc57c \ud568. \uae30\uc874 \uc5f0\uad6c\ub294 \ub2e8\uc21c\ud788 \ub370\uc774\ud130\uc151 \uc804\uccb4\uc758 text-following ratio\uac19\uc740 \uac70\uc2dc\uc801 \ud1b5\uacc4\ub85c \uc774\ub97c \uce21\uc815\ud588\uc74c. \uc774 \ubc29\uc2dd\uc740 \ubaa8\ub378\uc758 \uc9c4\uc9dc \ud3c9\ud5a5\uacfc unimodal confidence\uac00 \ub4a4\uc11e\uc5ec \ubaa8\ub378\uc758 \ud589\ub3d9\uc744 \uc624\ud574\ud558\uac8c \ub9cc\ub4ec."}),"\n",(0,t.jsx)(n.p,{children:"\uc608\ub97c \ub4e4\uc5b4\uc11c, \ubaa8\ub378\uc740 \uc0ac\uc2e4 \uc774\ubbf8\uc9c0 modality\ub97c \ub354 \uc120\ud638\ud558\uc9c0\ub9cc \uc774\ubbf8\uc9c0\uc5d0\uc11c \ubf51\uc740 \uc815\ubcf4\ub97c \ud655\uc2e0\ud558\uc9c0\ub294 \ubabb\ud560 \uc218 \uc788\uc74c. \uc774\ub7f0 \uacbd\uc6b0 \uc120\ud638\ub3c4\ub294 \ub0ae\uc544\ub3c4 \ud655\uc2e0\uc744 \uac15\ud558\uac8c \ud558\ub294 \ud14d\uc2a4\ud2b8 oriented \uc815\ub2f5\uc744 \ub0b4\ub193\uc744 \uac00\ub2a5\uc131\uc774 \ub192\ub2e4."}),"\n",(0,t.jsxs)(n.p,{children:["\uc774 \ub17c\ubb38\uc5d0\uc11c\ub294 ",(0,t.jsx)(n.strong,{children:"modality following"})," \ud589\ub3d9\uc744 ",(0,t.jsx)(n.strong,{children:"relative reasoning uncertainty"}),"\uc640 ",(0,t.jsx)(n.strong,{children:"inherent modality preference"}),"\ub77c\ub294 \ub450 \uac00\uc9c0 \ud575\uc2ec \uc694\uc778\uc73c\ub85c \ubd84\ub9ac\ud558\uc5ec \uc0dd\uac01\ud568. \uadf8 \ud6c4, \uc2dc\uac01\uc801 \ub09c\uc774\ub3c4\uc640 \ud14d\uc2a4\ud2b8 \ub09c\uc774\ub3c4\ub97c \ub3c5\ub9bd\uc801\uc73c\ub85c \uc81c\uc5b4\ud558\ub294 \ub370\uc774\ud130\uc151\uc744 \uad6c\ucd95, \ubaa8\ub378 \ucd9c\ub825 \ud1a0\ud070 \uc5d4\ud2b8\ub85c\ud53c\ub85c \ubd88\ud655\uc2e4\uc131\uc744 \uc815\ub7c9\ud654."]}),"\n",(0,t.jsx)(n.p,{children:"\uacb0\uacfc\uc801\uc73c\ub85c \ubaa8\ub4e0 MLLM\uc774 \ucd5c\uc885\uc801\uc73c\ub85c \ub0b4\ub193\ub294 \ub2f5\ubcc0\uc740 modal uncertainty\uc5d0 \uc758\uc874\ud55c\ub2e4\uace0 \ud655\uc778. \uc989, modal\uc774 \uc0c1\ub300\uc801\uc73c\ub85c \ubd88\ud655\uc2e4\ud560\uc218\ub85d \ub2e4\ub978 modal\uc744 \ub530\ub974\uac8c \ub41c\ub2e4."}),"\n",(0,t.jsx)(n.p,{children:"modal uncertainty\uac00 \ub3d9\uc77c\ud55c \uc9c0\uc810\uc5d0\uc11c\ub294 \ubaa8\ub378\uc758 \ub0b4\uc801 \uc120\ud638\ub3c4\ub97c \ud655\uc778\ud560 \uc218 \uc788\ub294\ub370, \ub300\ubd80\ubd84\uc758 \ubaa8\ub378\uc774 \uc774 \uc9c0\uc810\uc5d0\uc11c \ub0b4\ubd80 \ub808\uc774\uc5b4\ub9c8\ub2e4 \uc120\ud638\uac00 \ub2e4\ub974\uace0 \uc774\ub85c \uc778\ud574\uc11c \ubd88\uc548\uc815\ud55c \ub2f5\ubcc0\uc744 \ub0b4\ub193\uc558\ub2e4\uace0 \ud568."}),"\n",(0,t.jsxs)(n.h2,{id:"3-the-collaboration-gap-microsoft",children:["3. ",(0,t.jsx)(n.a,{href:"https://arxiv.org/pdf/2511.02687",children:"THE COLLABORATION GAP"})," ",(0,t.jsx)(n.code,{children:"microsoft"})]}),"\n",(0,t.jsx)(n.p,{children:"\ubbf8\ub798 AI \uc2dc\uc2a4\ud15c\uc740 collaboration among AI agents\uac00 \ud575\uc2ec. \ud604\uc7ac \ubca4\uce58\ub9c8\ud06c\ub294 AI\uac04 \ud611\uc5c5 \ub2a5\ub825\uc744 \ud3c9\uac00\ud560 \uc218 \uc5c6\uc73c\ubbc0\ub85c \ub9c8\uc774\ud06c\ub85c\uc18c\ud504\ud2b8\uc5d0\uc11c \uc81c\uc548."}),"\n",(0,t.jsx)(n.p,{children:"\ub450 agent\uc5d0\uac8c \ubd88\uc644\uc804\ud558\uace0 \uc0c1\ud638\ubcf4\uc644\uc801\uc778 \ubbf8\ub85c\ub97c \uc81c\uacf5. \uc18c\ud1b5\uc744 \ud574\uc57c \uacbd\ub85c\ub97c \ucc3e\uc744 \uc218 \uc788\uc74c. 2\uc778 \ud611\ub3d9 \uac8c\uc784\uac19\uc740 \uac78 \uc2dc\ucf30\ub2e4\uace0 \ubcf4\uba74 \ub420 \uac83 \uac19\uc74c."}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"collaboration gap"})," \ubc1c\uacac. \ud63c\uc790\uc11c \uc791\uc5c5\uc744 \uc798 \uc218\ud589\ud558\ub294 \ubaa8\ub378\uc774\ub354\ub77c\ub3c4 \ud611\uc5c5 \ud658\uacbd\uc5d0\uc11c\ub294 \uc131\ub2a5\uc774 \ud604\uc800\ud788 \uc800\ud558\ub428. \uc2ec\uc9c0\uc5b4 homogeneous agent\uc640\uc758 \ud611\uc5c5\ub3c4 \uc798 \ubabb\ud55c\ub2e4. \ud2b9\ud788 distilled model\uc5d0\uc11c \uc774 \uacbd\ud5a5\uc131\uc774 \ub354 \uac15\ud558\uac8c \ub098\ud0c0\ub0a8."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"ordering effects"})," \ubc1c\uacac. \ub204\uac00 \uba3c\uc800 \ub9d0\ud558\ub294\uc9c0\uac00 \uacb0\uacfc\uc5d0 \ud070 \uc601\ud5a5\uc744 \ubbf8\uce68. \uc774\ub97c \uc774\uc6a9\ud574\uc11c \uac15\ub825\ud55c \ubaa8\ub378\uc774 \ub300\ud654 \ucd08\ubc18\uc744 \uc8fc\ub3c4\ud558\ub294 ",(0,t.jsx)(n.strong,{children:"relay inference"}),"\ub97c \ud1b5\ud574 \ud611\uc5c5 \uc131\ub2a5\uc744 \uac1c\uc120."]}),"\n",(0,t.jsxs)(n.h2,{id:"4-can-visual-input-be-compressed",children:["4. ",(0,t.jsx)(n.a,{href:"https://huggingface.co/papers/2511.02650",children:"Can Visual Input Be Compressed?"})]}),"\n",(0,t.jsx)(n.h3,{id:"a-visual-token-compression-benchmark-for-large-multimodal-models",children:"A Visual Token Compression Benchmark for Large Multimodal Models"}),"\n",(0,t.jsxs)(n.p,{children:["visual token pruning \ubc29\ubc95 \ubd84\uc11d\ud55c \ub17c\ubb38. \ubd84\uc11d \uacb0\uacfc, \uc758\uc678\ub85c ",(0,t.jsx)(n.strong,{children:"random token pruning"}),"\uc774 \ub180\ub77c\uc6b8 \uc815\ub3c4\ub85c \uac15\ub825\ud55c \ubca0\uc774\uc2a4\ub77c\uc778\uc774\uc5c8\ub2e4\uace0 \ud568."]}),"\n",(0,t.jsx)("div",{style:{textAlign:"center"},children:(0,t.jsx)("img",{src:a,style:{width:500}})}),"\n",(0,t.jsx)(n.p,{children:"\ub2e4\ub9cc, OCR tawsk\ub294 pruning\uc5d0 \ub9e4\uc6b0 \ucde8\uc57d\ud568."}),"\n",(0,t.jsxs)(n.h2,{id:"5-ltd-bench-tencent",children:["5. ",(0,t.jsx)(n.a,{href:"https://huggingface.co/papers/2511.02347",children:"LTD-Bench"})," ",(0,t.jsx)(n.code,{children:"tencent"})]}),"\n",(0,t.jsx)(n.h3,{id:"evaluating-large-language-models-by-letting-them-draw",children:"Evaluating Large Language Models by Letting Them Draw"}),"\n",(0,t.jsx)(n.p,{children:"LLM \ud3c9\uac00\uc2dc, \uacf5\uac04 \ucd94\ub860 \ub2a5\ub825\uc740 numeric metrics \uac00 \uc544\ub2c8\ub77c, \uc9c1\uc811 \uad00\ucc30 \uac00\ub2a5\ud55c \uc2dc\uac01\uc801 \ucd9c\ub825\uc744 \ubcf4\uc790. \uc9c1\uad00\uc801\uc73c\ub85c \ud574\uc11d \uac00\ub2a5\u3139\ud55c \uc2dc\uac01\uc801 \uacb0\uacfc\ubb3c\ub85c \ubaa8\ub378 \ud3c9\uac00\ud558\uc790."}),"\n",(0,t.jsx)("div",{style:{textAlign:"center"},children:(0,t.jsx)("img",{src:o,style:{width:500}})}),"\n",(0,t.jsxs)(n.h2,{id:"6-reg-dpo-bytedance",children:["6. ",(0,t.jsx)(n.a,{href:"https://huggingface.co/papers/2511.01450",children:"Reg-DPO"})," ",(0,t.jsx)(n.code,{children:"bytedance"})]}),"\n",(0,t.jsx)(n.h3,{id:"sft-regularized-direct-preference-optimization-with-gt-pair-for-improving-video-generation",children:"SFT-Regularized Direct Preference Optimization with GT-Pair for Improving Video Generation"})]})}function u(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(p,{...e})}):p(e)}}}]);