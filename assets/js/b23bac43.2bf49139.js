"use strict";(globalThis.webpackChunkwebsite=globalThis.webpackChunkwebsite||[]).push([[8631],{1593:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>h,contentTitle:()=>c,default:()=>d,frontMatter:()=>l,metadata:()=>s,toc:()=>t});const s=JSON.parse('{"id":"papers/bigtech/2025-10","title":"oct.2025","description":"25\ub144 10\uc6d4\ub2ec \uad00\uc2ec \uac00\ub294 \ud68c\uc0ac\ub4e4\uc5d0\uc11c \ub0b8 \ub17c\ubb38. \ud2b9\ud788 \uc7ac\ubc0c\uac8c \ubcf8 \ud68c\uc0ac\ub4e4\uc740 \uc8fc\uc11d\uc744 \ub2ec\uc558\ub2e4.","source":"@site/docs/papers/bigtech/2025-10.md","sourceDirName":"papers/bigtech","slug":"/papers/bigtech/2025-10","permalink":"/docs/papers/bigtech/2025-10","draft":false,"unlisted":false,"editUrl":"https://github.com/logicbaron/logicbaron.github.io/tree/dev/docs/papers/bigtech/2025-10.md","tags":[],"version":"current","frontMatter":{},"sidebar":"BigtechSidebar"}');var r=i(4848),a=i(8453);const l={},c="oct.2025",h={},t=[{value:"Microsoft",id:"microsoft",level:2},{value:"NVIDIA",id:"nvidia",level:2},{value:"Google",id:"google",level:2},{value:"Amazon",id:"amazon",level:2},{value:"Meta",id:"meta",level:2},{value:"ByteDance",id:"bytedance",level:2},{value:"Adobe",id:"adobe",level:2},{value:"Apple",id:"apple",level:2},{value:"Huggingface",id:"huggingface",level:2},{value:"Alibaba",id:"alibaba",level:2},{value:"Tencent",id:"tencent",level:2},{value:"BAAI",id:"baai",level:2},{value:"Qualcomm",id:"qualcomm",level:2},{value:"Salesforce",id:"salesforce",level:2},{value:"Xiaomi",id:"xiaomi",level:2}];function o(e){const n={a:"a",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,a.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"oct2025",children:"oct.2025"})}),"\n",(0,r.jsx)(n.p,{children:"25\ub144 10\uc6d4\ub2ec \uad00\uc2ec \uac00\ub294 \ud68c\uc0ac\ub4e4\uc5d0\uc11c \ub0b8 \ub17c\ubb38. \ud2b9\ud788 \uc7ac\ubc0c\uac8c \ubcf8 \ud68c\uc0ac\ub4e4\uc740 \uc8fc\uc11d\uc744 \ub2ec\uc558\ub2e4."}),"\n",(0,r.jsx)(n.h2,{id:"microsoft",children:"Microsoft"}),"\n",(0,r.jsx)(n.p,{children:"\ub9c8\uc774\ud06c\ub85c\uc18c\ud504\ud2b8 \ub17c\ubb38\uc740 \uc8fc\uc81c\uac00 \ub2e4\uc591\ud55c \ud3b8. \uadf8\ub9ac\uace0 \ub17c\ubb38\ub4e4\uc774 \ud558\ub098\ud558\ub098 \uc5c4\uccad \uac1c\uc131\uc788\uace0 \uc7ac\ubc0c\ub2e4."}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://huggingface.co/papers/2510.13998",children:"BitNet Distillation"})," (10-18)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://huggingface.co/papers/2510.17498",children:"DEEP SELF-EVOLVING REASONING"})," (10-21)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://huggingface.co/papers/2510.19363",children:"LOONGRL: REINFORCEMENT LEARNING FOR ADVANCED REASONING OVER LONG CONTEXTS"})," (10-23)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://huggingface.co/papers/2510.24514",children:"Latent Sketchpad: Sketching Visual Thoughts to Elicit Multimodal Reasoning in MLLMs"})," (10-29)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://huggingface.co/papers/2510.25779",children:"Magentic Marketplace: An Open-Source Environment for Studying Agentic Markets"})," (10-31)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://huggingface.co/papers/2510.18212",children:"SWIREASONING: SWITCH-THINKING IN LATENT AND EXPLICIT FOR PARETO-SUPERIOR REASONING LLMS"})," (10-07)"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"nvidia",children:"NVIDIA"}),"\n",(0,r.jsx)(n.p,{children:"\uc5d4\ube44\ub514\uc544 \uc5ed\uc2dc \uc8fc\uc81c\uac00 \uaf64 \ub2e4\uc591\ud558\ub2e4. Fron-loading \ub17c\ubb38\uc774 \uac1c\uc778\uc801\uc73c\ub85c \uc778\uc0c1\uae4a\uc5c8\uc74c."}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://huggingface.co/papers/2510.13054",children:"VLA-0: Building State-of-the-Art VLAs with Zero Modification"})," (10-18)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://huggingface.co/papers/2510.15110",children:"DLER: Doing Length pEnalty Right: Incentivizing More Intelligence per Token via Reinforcement Learning"})," (10-21)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://huggingface.co/papers/2510.15870",children:"OmniVinci: Enhancing Architecture and Data for Omni-Modal Understanding LLM"})," (10-21)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://huggingface.co/papers/2510.18941",children:"PROFBENCH MULTI-DOMAIN RUBRICS REQUIRING PROFESSIONAL KNOWLEDGE TO ANSWER AND JUDGE"})," (10-23):"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://huggingface.co/papers/2510.19307",children:"Unified Reinforcement and Imitation Learning for Vision-Language Models"})," (10-23)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://huggingface.co/papers/2510.16917",children:"Long live: REAL-TIME INTERACTIVE LONG VIDEO GENERATION"})," (10-05)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://huggingface.co/papers/2510.18212",children:"Front-Loading Reasoning: The Synergy between Pretraining and Post-Training Data"})," (10-07)"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"google",children:"Google"}),"\n",(0,r.jsxs)(n.p,{children:["\uad6c\uae00 \ub17c\ubb38\uc740 ",(0,r.jsx)(n.strong,{children:"LLM \uad00\ub828 \uc11c\ube44\uc2a4 \uc0ac\uc6a9 \uacbd\ud5d8 \uac1c\uc120"}),"\uc774\ub77c\ub294 \ubc29\ud5a5\uc131\uc774 \ubcf4\uc774\ub294 \uac83 \uac19\ub2e4. \uac80\uc0c9 \ud488\uc9c8, inference, abuse \uad00\ub828 \ub17c\ubb38\ub4e4."]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://huggingface.co/papers/2510.13217",children:"LLM-GUIDED HIERARCHICAL RETRIEVAL"})," (10-18)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://huggingface.co/papers/2510.15831",children:"VISTA: A Test-Time Self-Improving Video Generation Agent"})," (10-21)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://huggingface.co/papers/2510.18554",children:"Extracting alignment data in open models"})," (10-22)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://huggingface.co/papers/2510.21057",children:"Soft Instruction De-escalation Defense"})," (10-27)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://huggingface.co/papers/2510.22037",children:"ATLAS: ADAPTIVE TRANSFER SCALING LAWS FOR MULTILINGUAL PRETRAINING, FINETUNING, AND DECODING THE CURSE OF MULTILINGUALITY"})," (10-29)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://huggingface.co/papers/2510.25992",children:"Supervised Reinforcement Learning: From Expert Trajectories to Step-wise Reasoning"})," (10-31)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://huggingface.co/papers/2510.18672",children:"Judging with Confidence: Calibrating Autoraters to Preference Distributions"})," (10-07)"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"amazon",children:"Amazon"}),"\n",(0,r.jsx)(n.p,{children:"\uc544\ub9c8\uc874\uc740 \uad6c\uae00\uacfc \ube44\uc2b7\ud558\uac8c \uc11c\ube44\uc2a4 \uacbd\ud5d8 \uac1c\uc120 \ubc29\ud5a5\uc758 \uc5f0\uad6c\ub4e4\uc774 \uc774\ubc88\uc5d0 \ubc1c\ud45c\ub418\uc5c8\ub2e4. \uad6c\uae00\uc740 gemini\ub97c \ud3ec\ud568\ud55c \uc9c1\uc811\uc801\uc778 \uc11c\ube44\uc2a4\ub97c \uace0\ub824\ud55c \uac83 \uac19\uace0 \uc544\ub9c8\uc874\uc740 \ud074\ub77c\uc6b0\ub4dc\uc5d0\uc11c \uc81c\uacf5\ud558\ub294 LLM \uad00\ub828 \uc11c\ube44\uc2a4\uac00 \uc774\uc720\uac00 \uc544\ub2d0\uae4c \uc2f6\ub2e4."}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://huggingface.co/papers/2510.16259",children:"DISTRACTOR INJECTION ATTACKS ON LARGE REASONING MODELS: CHARACTERIZATION AND DEFENSE"})," (10-21)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://huggingface.co/papers/2510.18245",children:"SCALING LAWS MEET MODEL ARCHITECTURE: TOWARD INFERENCE-EFFICIENT LLMS"})," (10-27)"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"meta",children:"Meta"}),"\n",(0,r.jsx)(n.p,{children:"\uac1c\uc778\uc801\uc73c\ub85c ray-ban meta \uc0ac\uc6a9\uacbd\ud5d8 \uad00\ub828\ub41c \ub17c\ubb38\ub4e4\uc774\ub77c\uace0 \uc0dd\uac01\ub428."}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://huggingface.co/papers/2510.11288",children:"Hybrid Architectures for Language Models: Systematic Analysis and Design Insights"})," (10-07)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://huggingface.co/papers/2510.17269",children:"HoneyBee: Data Recipes for Vision-Language Reasoners"})," (10-17)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://huggingface.co/papers/2510.16258",children:"Embody 3D: A Large-scale Multimodal Motion and Behavior Dataset"})," (10-21)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://huggingface.co/papers/2510.26160",children:"CRAG-MM: Multi-modal Multi-turn Comprehensive RAG Benchmark"})," (10-31)"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"bytedance",children:"ByteDance"}),"\n",(0,r.jsx)(n.p,{children:"\ub17c\ubb38\uc744 \uc3df\uc544\ub0b4\ub294\ub370 \ube44\ub514\uc624 \uad00\ub828 \ub17c\ubb38\uc758 \ube44\uc911\uc774 \ub192\ub2e4. bytedance\uac00 \uc774\ubbf8\uc9c0, \ube44\ub514\uc624 \uad00\ub828 \uc5ed\ub7c9\uc73c\ub85c world model, \ub354 \ub098\uc544\uac00\uc11c llm agent\ub97c \ud65c\uc6a9\ud55c \uac8c\uc784 \uc0b0\uc5c5\uc744 \uc0dd\uac01\ud558\uace0 \uc788\ub2e4\uace0 \uc0dd\uac01 \uc911."}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://huggingface.co/papers/2509.09658",children:"Towards General Agentic Intelligence via Environment Scaling"})," (09-22)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://huggingface.co/papers/2510.17437",children:"Self-Forcing++: Towards Minute-Scale High-Quality Video Generation"})," (10-04)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://huggingface.co/papers/2510.18245",children:"SAIL-Embedding: Omni-modal Embedding Foundation Model"})," (10-17)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://huggingface.co/papers/2510.18692",children:"MoGA: Mixture-of-Groups Attention for End-to-End Long Video Generation"})," (10-22)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://huggingface.co/papers/2510.18876",children:"Grasp Any Region: Towards Precise, Contextual Pixel Understanding for Multimodal LLMs"})," (10-22)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://huggingface.co/papers/2510.20579",children:"Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal Evidence"})," (10-24)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://huggingface.co/papers/2510.20888",children:"Video-As-Prompt: Unified Semantic Control for Video Generation"})," (10-27)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://huggingface.co/papers/2510.23691",children:"Game-TARS: Pretrained Foundation Models for Scalable Generalist Multimodal Game Agents"})," (10-29)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://huggingface.co/papers/2510.17439",children:"From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors"})," (10-29)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://huggingface.co/papers/2510.25741",children:"Scaling Latent Reasoning via Looped Language Models"})," (10-30)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://huggingface.co/papers/2510.24824",children:"Parallel Loop Transformer: Efficient Test-Time Computation Scaling"})," (10-30)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://huggingface.co/papers/2510.25682",children:"PairUni: Pairwise Training for Unified Multimodal Language Models"})," (10-30)"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"adobe",children:"Adobe"}),"\n",(0,r.jsx)(n.p,{children:"\uc774\ubbf8\uc9c0, \ube44\ub514\uc624 \ud3b8\uc9d1 \uad00\ub828 \ub17c\ubb38."}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"https://huggingface.co/papers/2510.17790",children:"UniFusion: Vision-Language Model as Unified Encoder in Image Generation"})," (10-17)"]}),"\n"]}),"\n",(0,r.jsxs)(n.li,{children:["\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.a,{href:"https://huggingface.co/papers/2510.14974",children:"PI-FLOW: POLICY-BASED FEW-STEP GENERATION VIA IMITATION DISTILLATION"})," (10-18)"]}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"apple",children:"Apple"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://huggingface.co/papers/2509.09658",children:"AToken: A Unified Tokenizer for Vision"})," (09-22)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://huggingface.co/papers/2510.18554",children:"DeepMMSearch-R1: Empowering Multimodal LLMs in Multimodal Web Search"})," (10-16)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://huggingface.co/papers/2510.17790",children:"UltraCUA: A Foundation Model for Computer Use Agents with Hybrid Action"})," (10-21)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://huggingface.co/papers/2510.19808",children:"Pico-Banana-400K: A Large-Scale Dataset for Text-Guided Image Editing"})," (10-23)"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"huggingface",children:"Huggingface"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://huggingface.co/papers/2510.17269",children:"FineVision: Open Data Is All You Need"})," (10-21)"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"alibaba",children:"Alibaba"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://huggingface.co/papers/2510.16917",children:"Winning the Pruning Gamble: A Unified Approach to Joint Sample and Token Pruning for Efficient Supervised Fine-Tuning"})," (10-05)"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"tencent",children:"Tencent"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://huggingface.co/papers/2510.14943",children:"LaSeR: Reinforcement Learning with Last-Token Self-Rewarding"})," (10-18)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://huggingface.co/papers/2510.20187",children:"Every Question Has Its Own Value: Reinforcement Learning with Explicit Human Values"})," (10-24)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://huggingface.co/papers/2510.18455",children:"ChronoPlay: A Framework for Modeling Dual Dynamics and Authenticity in Game RAG Benchmarks"})," (10-30)"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"baai",children:"BAAI"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://huggingface.co/papers/2510.25992",children:"Emu3.5: Native Multimodal Models are World Learners"})," (10-31)"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"qualcomm",children:"Qualcomm"}),"\n",(0,r.jsx)(n.p,{children:"\uc5e5"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://huggingface.co/papers/2510.17045",children:"VIDEO REASONING WITHOUT TRAINING"})," (10-22)"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"salesforce",children:"Salesforce"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://huggingface.co/papers/2510.17797",children:"Enterprise Deep Research: Steerable MultiAgent Deep Research for Enterprise Analytics"})," (10-21)"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"xiaomi",children:"Xiaomi"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://huggingface.co/papers/2510.11370",children:"Stabilizing MoE Reinforcement Learning: Aligning Training and Inference Routers"})," (10-27)"]}),"\n"]})]})}function d(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(o,{...e})}):o(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>l,x:()=>c});var s=i(6540);const r={},a=s.createContext(r);function l(e){const n=s.useContext(a);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:l(e.components),s.createElement(a.Provider,{value:n},e.children)}}}]);